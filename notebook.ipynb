{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c168d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c01befa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/File/shamgholi/projects/person_reid/AP3D\n"
     ]
    }
   ],
   "source": [
    "cd {cwd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a7f206",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec97ca76-3d72-4545-b529-51ae6a31fe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(cwd)\n",
    "import time\n",
    "import math\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from config import Config\n",
    "import models\n",
    "import transforms.spatial_transforms as ST\n",
    "import transforms.temporal_transforms as TT\n",
    "import tools.data_manager as data_manager\n",
    "from tools.video_loader import VideoDataset\n",
    "from tools.utils import Logger\n",
    "# from tools.eval_metrics import evaluate\n",
    "from commons import modify_model\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "def seed_everythings():\n",
    "    seed = 1\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "class Args:\n",
    "    #\"/home/hadi/iust/datasets\"\n",
    "    #\"/home/shamgholi/iust/datasets\" \n",
    "    #\"/mnt/File/shamgholi/datasets\"\n",
    "    root = \"/mnt/File/shamgholi/datasets\"\n",
    "    dataset = \"mars\"\n",
    "    workers = 0\n",
    "    height = 256\n",
    "    width = 128\n",
    "    # test_frames = 8\n",
    "    arch = \"ap3dres50\"\n",
    "    resume = \"./\"\n",
    "    pretrain = os.path.join(cwd, \"logs/row41/best_model.pth.tar\")\n",
    "    test_epochs = [240]\n",
    "    distance = \"cosine\" #cosine\n",
    "    gpu = \"0\"\n",
    "    test_batch = 6\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba059b87-e0f0-467f-b047-a9de6c832a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "def compute_ap_cmc(index, good_index, junk_index):\n",
    "    ap = 0\n",
    "    cmc = np.zeros(len(index)) \n",
    "    \n",
    "    # remove junk_index\n",
    "    mask = np.in1d(index, junk_index, invert=True)\n",
    "    index = index[mask]\n",
    "\n",
    "    # find good_index index\n",
    "    ngood = len(good_index)\n",
    "    mask = np.in1d(index, good_index)\n",
    "    rows_good = np.argwhere(mask==True)\n",
    "    rows_good = rows_good.flatten()\n",
    "    \n",
    "    cmc[rows_good[0]:] = 1.0\n",
    "    for i in range(ngood):\n",
    "        d_recall = 1.0/ngood\n",
    "        precision = (i+1)*1.0/(rows_good[i]+1)\n",
    "        ap = ap + d_recall*precision\n",
    "\n",
    "    return ap, cmc\n",
    "\n",
    "\n",
    "def evaluate(distmat, q_pids, g_pids, q_camids, g_camids, ret_all_t1=False):\n",
    "    num_q, num_g = distmat.shape\n",
    "    index = np.argsort(distmat, axis=1) # from small to large\n",
    "\n",
    "    num_no_gt = 0 # num of query imgs without groundtruth\n",
    "    num_r1 = 0\n",
    "    CMC = np.zeros(len(g_pids))\n",
    "    AP = 0\n",
    "    all_t1 = []\n",
    "    for i in range(num_q):\n",
    "        # groundtruth index\n",
    "        query_index = np.argwhere(g_pids==q_pids[i])\n",
    "        camera_index = np.argwhere(g_camids==q_camids[i])\n",
    "        good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n",
    "        if good_index.size == 0:\n",
    "            num_no_gt += 1\n",
    "            continue\n",
    "        # remove gallery samples that have the same pid and camid with query\n",
    "        junk_index = np.intersect1d(query_index, camera_index)\n",
    "\n",
    "        ap_tmp, CMC_tmp = compute_ap_cmc(index[i], good_index, junk_index)\n",
    "        if CMC_tmp[0]==1:\n",
    "            num_r1 += 1\n",
    "        all_t1.append(CMC_tmp[0])\n",
    "        CMC = CMC + CMC_tmp\n",
    "        AP += ap_tmp\n",
    "\n",
    "    if num_no_gt > 0:\n",
    "        print(\"{} query imgs do not have groundtruth.\".format(num_no_gt))\n",
    "\n",
    "    # print(\"R1:{}\".format(num_r1))\n",
    "\n",
    "    CMC = CMC / (num_q - num_no_gt)\n",
    "    mAP = AP / (num_q - num_no_gt)\n",
    "    if ret_all_t1:\n",
    "        return CMC, mAP, all_t1\n",
    "    else:\n",
    "        return CMC, mAP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c77a8-d3df-40cc-b7ea-bd117c918e2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Default Test (Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9094fe3-7444-4f9d-91c1-83838120a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, queryloader, galleryloader, use_gpu, args):\n",
    "    # test using 4 frames\n",
    "    since = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    qf, q_pids, q_camids = [], [], []\n",
    "    for batch_idx, (vids, pids, camids) in enumerate(queryloader):\n",
    "        if use_gpu:\n",
    "            vids = vids.cuda()\n",
    "\n",
    "        feat = model(vids)\n",
    "        feat = feat.mean(1)\n",
    "        feat = model.bn(feat)\n",
    "        feat = feat.data.cpu()\n",
    "\n",
    "        qf.append(feat)\n",
    "        q_pids.extend(pids)\n",
    "        q_camids.extend(camids)\n",
    "\n",
    "    qf = torch.cat(qf, 0)\n",
    "    q_pids = np.asarray(q_pids)\n",
    "    q_camids = np.asarray(q_camids)\n",
    "    print(\"Extracted features for query set, obtained {} matrix\".format(qf.shape))\n",
    "\n",
    "    gf, g_pids, g_camids = [], [], []\n",
    "    for batch_idx, (vids, pids, camids) in enumerate(galleryloader):\n",
    "        if use_gpu:\n",
    "            vids = vids.cuda()\n",
    "\n",
    "        feat = model(vids)\n",
    "        feat = feat.mean(1)\n",
    "        feat = model.bn(feat)\n",
    "        feat = feat.data.cpu()\n",
    "\n",
    "        gf.append(feat)\n",
    "        g_pids.extend(pids)\n",
    "        g_camids.extend(camids)\n",
    "\n",
    "    gf = torch.cat(gf, 0)\n",
    "    g_pids = np.asarray(g_pids)\n",
    "    g_camids = np.asarray(g_camids)\n",
    "\n",
    "    if args.dataset == 'mars':\n",
    "        # gallery set must contain query set, otherwise 140 query imgs will not have ground truth.\n",
    "        gf = torch.cat((qf, gf), 0)\n",
    "        g_pids = np.append(q_pids, g_pids)\n",
    "        g_camids = np.append(q_camids, g_camids)\n",
    "\n",
    "    print(\"Extracted features for gallery set, obtained {} matrix\".format(gf.shape))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Extracting features complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print(\"Computing distance matrix\")\n",
    "    m, n = qf.size(0), gf.size(0)\n",
    "    distmat = torch.zeros((m,n))\n",
    "\n",
    "    if args.distance == 'euclidean':\n",
    "        distmat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n",
    "                  torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n",
    "        for i in range(m):\n",
    "            distmat[i:i+1].addmm_(1, -2, qf[i:i+1], gf.t())\n",
    "    else:\n",
    "        q_norm = torch.norm(qf, p=2, dim=1, keepdim=True)\n",
    "        g_norm = torch.norm(gf, p=2, dim=1, keepdim=True)\n",
    "        qf = qf.div(q_norm.expand_as(qf))\n",
    "        gf = gf.div(g_norm.expand_as(gf))\n",
    "        for i in range(m):\n",
    "            distmat[i] = - torch.mm(qf[i:i+1], gf.t())\n",
    "    distmat = distmat.numpy()\n",
    "    \n",
    "    print(\"Computing CMC and mAP\")\n",
    "    cmc, mAP = evaluate(distmat, q_pids, g_pids, q_camids, g_camids)\n",
    "\n",
    "    print(\"Results ----------\")\n",
    "    print('top1:{:.1%} top5:{:.1%} top10:{:.1%} mAP:{:.1%}'.format(cmc[0],cmc[4],cmc[9],mAP))\n",
    "    print(\"------------------\")\n",
    "\n",
    "    return cmc[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def experiment():\n",
    "    print(\"Initializing model: {}\".format(args.arch))\n",
    "    seed_everythings()\n",
    "    model = models.init_model(name=args.arch, conf=conf, num_classes=dataset.num_train_pids)\n",
    "    print(\"Model size: {:.5f}M\".format(sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    modify_model(model, args, conf)\n",
    "    with torch.no_grad():\n",
    "        test(model, queryloader, galleryloader, use_gpu, args)\n",
    "        # mytest(model, queryloader, galleryloader, use_gpu, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57898897-3e93-41d3-86e9-017fd3b4241c",
   "metadata": {},
   "source": [
    "# Prepare Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dae5896-5f30-4187-8b4d-2d3d70bfdd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> MARS loaded\n",
      "Dataset statistics:\n",
      "  ------------------------------\n",
      "  subset   | # ids | # tracklets\n",
      "  ------------------------------\n",
      "  train    |   625 |     8298\n",
      "  query    |   626 |     1980\n",
      "  gallery  |   622 |     9330\n",
      "  ------------------------------\n",
      "  total    |  1247 |    19608\n",
      "  number of images per tracklet: 2 ~ 920, average 59.5\n",
      "  ------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation\n",
    "args = Args()\n",
    "conf = Config()\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = 'cuda' if use_gpu else 'cpu'\n",
    "pin_memory = True if use_gpu else False\n",
    "temporal_transform_test = TT.TemporalBeginCrop()\n",
    "dataset = data_manager.init_dataset(name=args.dataset, root=args.root)\n",
    "spatial_transform_test = ST.Compose([\n",
    "            ST.Scale((args.height, args.width), interpolation=3),\n",
    "            ST.ToTensor(),\n",
    "            ST.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "# return queryloader, galleryloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c67f9-376a-4b14-8795-8f44be615a19",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Base model Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f756ef37-1de9-4af9-be94-de55fd1aec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_transform_train = conf.get_spatial_transform_train(args)\n",
    "temporal_transform_train = TT.TemporalRandomCrop()\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    VideoDataset(dataset.train, spatial_transform=spatial_transform_train, temporal_transform=temporal_transform_test),\n",
    "    batch_size=32, num_workers=args.workers,\n",
    "    pin_memory=pin_memory, drop_last=True)\n",
    "\n",
    "\n",
    "conf.use_linear_to_get_important_features = False\n",
    "conf.print_model_parameters_trainable = False\n",
    "conf.use_linear_to_merge_features = False\n",
    "conf.use_hist = False\n",
    "args.pretrain = 'logs/row41/best_model.pth.tar'\n",
    "conf.print_model_layers = False\n",
    "conf.use_hist_and_max_seprately = False\n",
    "\n",
    "print(\"Initializing model: {}\".format(args.arch))\n",
    "seed_everythings()\n",
    "model = models.init_model(name=args.arch, conf=conf, num_classes=dataset.num_train_pids)\n",
    "print(\"Model size: {:.5f}M\".format(sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "modify_model(model, args, conf)\n",
    "with torch.no_grad():\n",
    "\n",
    "    # test using 4 frames\n",
    "    since = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    tf, t_pids, t_camids = [], [], []\n",
    "    for batch_idx, (vids, pids, camids) in tqdm(enumerate(trainloader)):\n",
    "        if use_gpu:\n",
    "            vids = vids.cuda()\n",
    "\n",
    "        feat = model(vids)\n",
    "        feat = feat.mean(1)\n",
    "        feat = model.bn(feat)\n",
    "        feat = feat.data.cpu()\n",
    "\n",
    "        tf.append(feat)\n",
    "        t_pids.extend(pids)\n",
    "        t_camids.extend(camids)\n",
    "        \n",
    "    tf = torch.cat(tf, 0)\n",
    "    t_pids = np.asarray(t_pids)\n",
    "    t_camids = np.asarray(t_camids)\n",
    "    print(\"Extracted features for query set, obtained {} matrix\".format(tf.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4f4a5-673d-4e5f-87b7-c9e8e9f30280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# res = Counter(t_pids) # equals to list(set(words))\n",
    "# # for k,v in list(zip(res.keys(), res.values()))[:5]:\n",
    "# #     print(k, v)\n",
    "# # res.most_common(10)\n",
    "# [(l,k) for k,l in sorted([(j,i) for i,j in res.items()], reverse=False)][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5881a3-78f6-47ec-ae00-91cc76337e82",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Train logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37dc2ae-d171-497a-9190-8f871c0c2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_dataset = []\n",
    "logistic_labels = []\n",
    "t_qids_unique = np.unique(t_pids)\n",
    "for i, tid in tqdm(enumerate(t_pids)):\n",
    "    population = [ind for ind, pid in enumerate(t_pids) if pid == tid]\n",
    "    population.remove(i)\n",
    "    # if len(population) > 3:\n",
    "    #     k = 4\n",
    "    # elif len(population) > 2:\n",
    "    #     k = 3\n",
    "    # elif len(population) > 1:\n",
    "    #     k = 2\n",
    "    # else:\n",
    "    #     k = 1\n",
    "    try:\n",
    "        i_sample = random.sample(population=population, k=1)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    feats = tf[i_sample]\n",
    "    for feat in feats:\n",
    "        logistic_dataset.append(abs(feat - tf[i]).unsqueeze(0))\n",
    "        logistic_labels.append(1.0)\n",
    "\n",
    "        \n",
    "for i, tid in tqdm(enumerate(t_pids)):\n",
    "    population = [ind for ind, pid in enumerate(t_pids) if pid != tid]\n",
    "    # if len(population) > 3:\n",
    "    #     k = 4\n",
    "    # elif len(population) > 2:\n",
    "    #     k = 3\n",
    "    # elif len(population) > 1:\n",
    "    #     k = 2\n",
    "    # else:\n",
    "    #     k = 1\n",
    "    try:\n",
    "        i_sample = random.sample(population=population, k=1)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    feats = tf[i_sample]\n",
    "    for feat in feats:\n",
    "        logistic_dataset.append(abs(feat - tf[i]).unsqueeze(0))\n",
    "        logistic_labels.append(0.0)\n",
    "      \n",
    "    \n",
    "    \n",
    "logistic_dataset = torch.cat(logistic_dataset, 0).numpy()\n",
    "logistic_labels = torch.FloatTensor(logistic_labels).numpy()\n",
    "print(logistic_dataset.shape)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.utils import shuffle\n",
    "logistic_dataset_shuff, logistic_labels_shuff = shuffle(logistic_dataset, logistic_labels)\n",
    "\n",
    "x_train, y_train = logistic_dataset_shuff[2000:], logistic_labels_shuff[2000:]\n",
    "x_val, y_val = logistic_dataset_shuff[:2000], logistic_labels_shuff[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2bb4a2-3725-4dd9-8207-c2d28ac2dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = svm.SVC(probability=True)\n",
    "# clf = LogisticRegression(random_state=1)\n",
    "clf.fit(x_train, y_train)\n",
    "clf.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68015bb-d42a-4143-83d1-331defffb1ed",
   "metadata": {},
   "source": [
    "# prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "699942ad-62f2-48b0-bce7-fc8430aa1144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# random.shuffle(dataset.query)\n",
    "# query_train = dataset.query[:900]\n",
    "# query_test = dataset.query[900:]\n",
    "\n",
    "queryloader = DataLoader(\n",
    "    VideoDataset(dataset.query, spatial_transform=spatial_transform_test, temporal_transform=temporal_transform_test),\n",
    "    batch_size=args.test_batch, shuffle=False, num_workers=2,\n",
    "    pin_memory=pin_memory, drop_last=False)\n",
    "galleryloader = DataLoader(\n",
    "    VideoDataset(dataset.gallery, spatial_transform=spatial_transform_test, temporal_transform=temporal_transform_test),\n",
    "    batch_size=args.test_batch, shuffle=False, num_workers=2,\n",
    "    pin_memory=pin_memory, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf019d0-667a-4e84-ae23-98a34a4c9fc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# test set feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e320a3db-adde-429b-92dc-3869919c960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_feature(model, queryloader, galleryloader, use_bn=True):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # test using 4 frames\n",
    "        since = time.time()\n",
    "        model.eval()\n",
    "\n",
    "        qf, q_pids, q_camids = [], [], []\n",
    "        for batch_idx, (vids, pids, camids) in enumerate(queryloader):\n",
    "            if use_gpu:\n",
    "                vids = vids.cuda()\n",
    "\n",
    "            feat = model(vids)\n",
    "            feat = feat.mean(1)\n",
    "            if use_bn:\n",
    "                feat = model.bn(feat)\n",
    "            feat = feat.data.cpu()\n",
    "\n",
    "            qf.append(feat)\n",
    "            q_pids.extend(pids)\n",
    "            q_camids.extend(camids)\n",
    "\n",
    "        qf = torch.cat(qf, 0)\n",
    "        q_pids = np.asarray(q_pids)\n",
    "        q_camids = np.asarray(q_camids)\n",
    "        print(\"Extracted features for query set, obtained {} matrix\".format(qf.shape))\n",
    "\n",
    "        gf, g_pids, g_camids = [], [], []\n",
    "        for batch_idx, (vids, pids, camids) in enumerate(galleryloader):\n",
    "            if use_gpu:\n",
    "                vids = vids.cuda()\n",
    "\n",
    "            feat = model(vids)\n",
    "            feat = feat.mean(1)\n",
    "            if use_bn:\n",
    "                feat = model.bn(feat)\n",
    "            feat = feat.data.cpu()\n",
    "\n",
    "            gf.append(feat)\n",
    "            g_pids.extend(pids)\n",
    "            g_camids.extend(camids)\n",
    "\n",
    "        gf = torch.cat(gf, 0)\n",
    "        g_pids = np.asarray(g_pids)\n",
    "        g_camids = np.asarray(g_camids)\n",
    "\n",
    "        if args.dataset == 'mars':\n",
    "            # gallery set must contain query set, otherwise 140 query imgs will not have ground truth.\n",
    "            gf = torch.cat((qf, gf), 0)\n",
    "            g_pids = np.append(q_pids, g_pids)\n",
    "            g_camids = np.append(q_camids, g_camids)\n",
    "\n",
    "        print(\"Extracted features for gallery set, obtained {} matrix\".format(gf.shape))\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Extracting features complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    return qf, q_pids, q_camids, gf, g_pids, g_camids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08d08cd2-76c6-4a12-a13f-ba561cf458e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model: ap3dres50\n",
      "Model size: 11.75930M\n",
      "pretrain state dict loaded\n",
      "----------\n",
      "Model size: 11.75930M\n"
     ]
    }
   ],
   "source": [
    "# query_train_loader = DataLoader(\n",
    "#     VideoDataset(query_train, spatial_transform=spatial_transform_test, temporal_transform=temporal_transform_test),\n",
    "#     batch_size=args.test_batch, shuffle=False, num_workers=0,\n",
    "#     pin_memory=pin_memory, drop_last=False)\n",
    "\n",
    "conf.use_linear_to_get_important_features = False\n",
    "conf.print_model_parameters_trainable = False\n",
    "conf.use_linear_to_merge_features = False\n",
    "conf.use_hist = False\n",
    "args.pretrain = 'logs/row41/best_model.pth.tar'\n",
    "conf.print_model_layers = False\n",
    "\n",
    "print(\"Initializing model: {}\".format(args.arch))\n",
    "seed_everythings()\n",
    "model = models.init_model(name=args.arch, conf=conf, num_classes=dataset.num_train_pids)\n",
    "print(\"Model size: {:.5f}M\".format(sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "modify_model(model, args, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d87877d-70f6-4a73-bf0a-fe8f95564c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features for query set, obtained torch.Size([1980, 512]) matrix\n",
      "Extracted features for gallery set, obtained torch.Size([11310, 512]) matrix\n",
      "Extracting features complete in 0m 44s\n",
      "CPU times: user 40.7 s, sys: 11.8 s, total: 52.5 s\n",
      "Wall time: 44.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qf, q_pids, q_camids, gf, g_pids, g_camids = extract_test_feature(model, queryloader, galleryloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f6ce6-ea82-4460-aa6d-9c1bbfc872e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## base model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16ea9f78-cf9d-40e9-824b-f48a619792aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing distance matrix\n",
      "Computing CMC and mAP\n",
      "Results ----------\n",
      "top1:83.5% top5:93.1% top10:95.1% mAP:73.2%\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing distance matrix\")\n",
    "m, n = qf.size(0), gf.size(0)\n",
    "distmat = torch.zeros((m,n))\n",
    "\n",
    "if args.distance == 'euclidean':\n",
    "    distmat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n",
    "              torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n",
    "    for i in range(m):\n",
    "        distmat[i:i+1].addmm_(1, -2, qf[i:i+1], gf.t())\n",
    "else:\n",
    "    q_norm = torch.norm(qf, p=2, dim=1, keepdim=True)\n",
    "    g_norm = torch.norm(gf, p=2, dim=1, keepdim=True)\n",
    "    qf = qf.div(q_norm.expand_as(qf))\n",
    "    gf = gf.div(g_norm.expand_as(gf))\n",
    "    for i in range(m):\n",
    "        distmat[i] = - torch.mm(qf[i:i+1], gf.t())\n",
    "distmat = distmat.numpy()\n",
    "\n",
    "print(\"Computing CMC and mAP\")\n",
    "cmc, mAP, t1s_base = evaluate(distmat, q_pids, g_pids, q_camids, g_camids, ret_all_t1=True)\n",
    "\n",
    "print(\"Results ----------\")\n",
    "print('top1:{:.1%} top5:{:.1%} top10:{:.1%} mAP:{:.1%}'.format(cmc[0],cmc[4],cmc[9],mAP))\n",
    "print(\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d2ec89-9655-4c98-81dd-e802ccffdabe",
   "metadata": {},
   "source": [
    "# HistByProfMultiChannel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "67c09a5a-533e-4427-8cf0-b7da2c548d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model: ap3dres50\n",
      "Model size: 13.68852M\n",
      "pretrain state dict loaded\n",
      "----------\n",
      "Model size: 13.68852M\n"
     ]
    }
   ],
   "source": [
    "conf.use_linear_to_get_important_features = False\n",
    "conf.print_model_parameters_trainable = False\n",
    "conf.use_linear_to_merge_features = False\n",
    "conf.use_hist = True\n",
    "args.pretrain = 'logs/row65/best_model.pth.tar'\n",
    "conf.print_model_layers = False\n",
    "\n",
    "print(\"Initializing model: {}\".format(args.arch))\n",
    "seed_everythings()\n",
    "model = models.init_model(name=args.arch, conf=conf, num_classes=dataset.num_train_pids)\n",
    "print(\"Model size: {:.5f}M\".format(sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "modify_model(model, args, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29050c54-9f9e-49d5-9523-a2da368ba986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features for query set, obtained torch.Size([1980, 3584]) matrix\n",
      "Extracted features for gallery set, obtained torch.Size([11310, 3584]) matrix\n",
      "Extracting features complete in 0m 51s\n",
      "CPU times: user 4min 44s, sys: 25.7 s, total: 5min 10s\n",
      "Wall time: 50.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qf12, q_pids12, q_camids12, gf12, g_pids12, g_camids12 = extract_test_feature(model, queryloader, galleryloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2024e751-1fc8-4424-80bb-425a5e5e5e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing distance matrix\n",
      "Computing CMC and mAP\n",
      "Results ----------\n",
      "top1:82.7% top5:93.0% top10:94.8% mAP:71.6%\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing distance matrix\")\n",
    "m, n = qf12.size(0), gf12.size(0)\n",
    "distmat = torch.zeros((m,n))\n",
    "\n",
    "q_norm12 = torch.norm(qf12, p=2, dim=1, keepdim=True)\n",
    "g_norm12 = torch.norm(gf12, p=2, dim=1, keepdim=True)\n",
    "qf12 = qf12.div(q_norm12.expand_as(qf12))\n",
    "gf12 = gf12.div(g_norm12.expand_as(gf12))\n",
    "for i in range(m):\n",
    "    distmat[i] = - torch.mm(qf12[i:i+1], gf12.t())\n",
    "distmat = distmat.numpy()\n",
    "\n",
    "print(\"Computing CMC and mAP\")\n",
    "cmc, mAP, t1s_hist = evaluate(distmat, q_pids12, g_pids12, q_camids12, g_camids12, ret_all_t1=True)\n",
    "\n",
    "print(\"Results ----------\")\n",
    "print('top1:{:.1%} top5:{:.1%} top10:{:.1%} mAP:{:.1%}'.format(cmc[0],cmc[4],cmc[9],mAP))\n",
    "print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9dfd1cb2-e2a7-487e-a793-7aae906ef5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "1596\n",
      "42\n",
      "284\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "tb_fh = []\n",
    "tb_th = []\n",
    "fb_th = []\n",
    "fb_fh = []\n",
    "for i, (h, b) in enumerate(list(zip(t1s_hist, t1s_base))):\n",
    "    if h == 1 and b == 1:\n",
    "        tb_th.append(i)\n",
    "    elif h == 0 and b == 1:\n",
    "        tb_fh.append(i)\n",
    "    elif h == 0 and b == 0:\n",
    "        fb_fh.append(i)\n",
    "    else:\n",
    "        fb_th.append(i)\n",
    "        \n",
    "print(len(tb_fh))\n",
    "print(len(tb_th))\n",
    "print(len(fb_th))\n",
    "print(len(fb_fh))\n",
    "random.seed(12345)\n",
    "sample_tb_fh = random.sample(tb_fh, k=40)\n",
    "sample_tb_th = random.sample(tb_th, k=40)\n",
    "sample_fb_th = random.sample(fb_th, k=40)\n",
    "sample_fb_fh = random.sample(fb_fh, k=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c2bc9c-183f-44ae-a2f5-c32f31494463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "dir_path = '/mnt/File/shamgholi/projects/person_reid/experiment/tb_fh/'\n",
    "for i in range(40):\n",
    "    img_paths, _, _ = dataset.query[sample_tb_fh[i]]\n",
    "    img_paths = temporal_transform_test(img_paths)\n",
    "    imgs = []\n",
    "    for imgpath in img_paths:\n",
    "        img = cv2.imread(imgpath)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # print(img.shape)\n",
    "        imgs.append(img)\n",
    "    plt.figure()\n",
    "    plt.axis('off')\n",
    "    plt.imshow(cv2.hconcat(imgs))    \n",
    "    plt.savefig(os.path.join(dir_path, f'{str(i+1).zfill(2)}.jpg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "672821e0-4de9-4f87-a034-317369553470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([304.,  28.,  36.,  34.,  41.,  25.,  21.,  14.,   4.,   5.]),\n",
       " array([0.65812546, 0.6759444 , 0.6937634 , 0.7115823 , 0.7294013 ,\n",
       "        0.7472202 , 0.76503915, 0.78285813, 0.80067706, 0.81849605,\n",
       "        0.836315  ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAoBUlEQVR4nO3dfXBUVZ7/8U8SSPOU7myApJMlQUB5GgLDBAk9MMhKlhAYxSHWADKALgMLJq6SGYeJpSDMLKFkanDHUtipQnBrQByqRlxRQR4EZAkg2coiIFkSMwYHOkHYpHkY8kDO/rE/7s+W8JCQNifh/aq6Zfqe06fPl5vc/nj73tthxhgjAAAAi4S39AQAAAC+iYACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALBOu5aeQFPU19fr9OnTioqKUlhYWEtPBwAA3AZjjC5cuKCEhASFh9/8GEmrDCinT59WYmJiS08DAAA0walTp9SjR4+b9mmVASUqKkrS/xXodrtbeDYAAOB2BAIBJSYmOu/jN9MqA8q1j3XcbjcBBQCAVuZ2Ts/gJFkAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA67Rr6QnY6J5fvtfSU2i0Py+f2NJTAACg2TTqCMqqVas0ePBgud1uud1u+Xw+ffDBB077lStXlJWVpa5du6pLly7KzMxUeXl50BhlZWWaOHGiOnXqpNjYWD377LOqq6trnmoAAECb0KiA0qNHDy1fvlwFBQU6fPiwHnzwQU2aNEnHjh2TJC1YsEDvvvuuNm3apD179uj06dOaPHmy8/yrV69q4sSJqqmp0f79+/XGG29o3bp1WrRoUfNWBQAAWrUwY4y5kwFiYmK0YsUKPfroo+revbs2bNigRx99VJJ04sQJDRgwQPn5+RoxYoQ++OAD/fCHP9Tp06cVFxcnSVq9erUWLlyos2fPKjIy8rZeMxAIyOPxqKqqSm63+06m3yA+4gEAoPk15v27ySfJXr16VRs3btSlS5fk8/lUUFCg2tpapaWlOX369++vpKQk5efnS5Ly8/OVnJzshBNJSk9PVyAQcI7CNKS6ulqBQCBoAQAAbVejA8qnn36qLl26yOVyad68eXr77bc1cOBA+f1+RUZGKjo6Oqh/XFyc/H6/JMnv9weFk2vt19puJC8vTx6Px1kSExMbO20AANCKNDqg9OvXT4WFhTp48KDmz5+vWbNm6fjx46GYmyM3N1dVVVXOcurUqZC+HgAAaFmNvsw4MjJS9957ryQpJSVFn3zyif7lX/5FU6ZMUU1NjSorK4OOopSXl8vr9UqSvF6vDh06FDTetat8rvVpiMvlksvlauxUAQBAK3XHN2qrr69XdXW1UlJS1L59e+3cudNpKyoqUllZmXw+nyTJ5/Pp008/VUVFhdNn+/btcrvdGjhw4J1OBQAAtBGNOoKSm5urjIwMJSUl6cKFC9qwYYN2796tbdu2yePxaPbs2crJyVFMTIzcbreeeuop+Xw+jRgxQpI0btw4DRw4UDNmzNBLL70kv9+v559/XllZWRwhAQAAjkYFlIqKCs2cOVNnzpyRx+PR4MGDtW3bNv393/+9JGnlypUKDw9XZmamqqurlZ6ertdee815fkREhLZs2aL58+fL5/Opc+fOmjVrlpYuXdq8VQEAgFbtju+D0hK4D8r1uA8KAMB238p9UAAAAEKFgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGCdRgWUvLw83X///YqKilJsbKweeeQRFRUVBfUZM2aMwsLCgpZ58+YF9SkrK9PEiRPVqVMnxcbG6tlnn1VdXd2dVwMAANqEdo3pvGfPHmVlZen+++9XXV2dnnvuOY0bN07Hjx9X586dnX5z5szR0qVLncedOnVyfr569aomTpwor9er/fv368yZM5o5c6bat2+vZcuWNUNJAACgtWtUQNm6dWvQ43Xr1ik2NlYFBQUaPXq0s75Tp07yer0NjvHhhx/q+PHj2rFjh+Li4vTd735Xv/rVr7Rw4UK9+OKLioyMbEIZAACgLbmjc1CqqqokSTExMUHr169fr27dumnQoEHKzc3V5cuXnbb8/HwlJycrLi7OWZeenq5AIKBjx441+DrV1dUKBAJBCwAAaLsadQTl6+rr6/XMM89o5MiRGjRokLP+scceU8+ePZWQkKAjR45o4cKFKioq0p/+9CdJkt/vDwonkpzHfr+/wdfKy8vTkiVLmjpVAADQyjQ5oGRlZeno0aPat29f0Pq5c+c6PycnJys+Pl5jx45VSUmJ+vTp06TXys3NVU5OjvM4EAgoMTGxaRMHAADWa9JHPNnZ2dqyZYs++ugj9ejR46Z9U1NTJUnFxcWSJK/Xq/Ly8qA+1x7f6LwVl8slt9sdtAAAgLarUQHFGKPs7Gy9/fbb2rVrl3r16nXL5xQWFkqS4uPjJUk+n0+ffvqpKioqnD7bt2+X2+3WwIEDGzMdAADQRjXqI56srCxt2LBB77zzjqKiopxzRjwejzp27KiSkhJt2LBBEyZMUNeuXXXkyBEtWLBAo0eP1uDBgyVJ48aN08CBAzVjxgy99NJL8vv9ev7555WVlSWXy9X8FQIAgFanUUdQVq1apaqqKo0ZM0bx8fHO8tZbb0mSIiMjtWPHDo0bN079+/fXz372M2VmZurdd991xoiIiNCWLVsUEREhn8+nn/zkJ5o5c2bQfVMAAMDdrVFHUIwxN21PTEzUnj17bjlOz5499f777zfmpQEAwF2E7+IBAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYp1EBJS8vT/fff7+ioqIUGxurRx55REVFRUF9rly5oqysLHXt2lVdunRRZmamysvLg/qUlZVp4sSJ6tSpk2JjY/Xss8+qrq7uzqsBAABtQqMCyp49e5SVlaUDBw5o+/btqq2t1bhx43Tp0iWnz4IFC/Tuu+9q06ZN2rNnj06fPq3Jkyc77VevXtXEiRNVU1Oj/fv364033tC6deu0aNGi5qsKAAC0amHGGNPUJ589e1axsbHas2ePRo8eraqqKnXv3l0bNmzQo48+Kkk6ceKEBgwYoPz8fI0YMUIffPCBfvjDH+r06dOKi4uTJK1evVoLFy7U2bNnFRkZecvXDQQC8ng8qqqqktvtbur0b+ieX77X7GOG2p+XT2zpKQAAcFONef++o3NQqqqqJEkxMTGSpIKCAtXW1iotLc3p079/fyUlJSk/P1+SlJ+fr+TkZCecSFJ6eroCgYCOHTvW4OtUV1crEAgELQAAoO1qckCpr6/XM888o5EjR2rQoEGSJL/fr8jISEVHRwf1jYuLk9/vd/p8PZxca7/W1pC8vDx5PB5nSUxMbOq0AQBAK9DkgJKVlaWjR49q48aNzTmfBuXm5qqqqspZTp06FfLXBAAALaddU56UnZ2tLVu2aO/everRo4ez3uv1qqamRpWVlUFHUcrLy+X1ep0+hw4dChrv2lU+1/p8k8vlksvlaspUAQBAK9SoIyjGGGVnZ+vtt9/Wrl271KtXr6D2lJQUtW/fXjt37nTWFRUVqaysTD6fT5Lk8/n06aefqqKiwumzfft2ud1uDRw48E5qAQAAbUSjjqBkZWVpw4YNeueddxQVFeWcM+LxeNSxY0d5PB7Nnj1bOTk5iomJkdvt1lNPPSWfz6cRI0ZIksaNG6eBAwdqxowZeumll+T3+/X8888rKyuLoyQAAEBSIwPKqlWrJEljxowJWr927Vo9/vjjkqSVK1cqPDxcmZmZqq6uVnp6ul577TWnb0REhLZs2aL58+fL5/Opc+fOmjVrlpYuXXpnlQAAgDbjju6D0lK4D8r1uA8KAMB239p9UAAAAEKBgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6jQ4oe/fu1UMPPaSEhASFhYVp8+bNQe2PP/64wsLCgpbx48cH9Tl//rymT58ut9ut6OhozZ49WxcvXryjQgAAQNvR6IBy6dIlDRkyRK+++uoN+4wfP15nzpxxljfffDOoffr06Tp27Ji2b9+uLVu2aO/evZo7d27jZw8AANqkdo19QkZGhjIyMm7ax+Vyyev1Ntj22WefaevWrfrkk080bNgwSdIrr7yiCRMm6De/+Y0SEhIaOyUAANDGhOQclN27dys2Nlb9+vXT/Pnzde7cOactPz9f0dHRTjiRpLS0NIWHh+vgwYOhmA4AAGhlGn0E5VbGjx+vyZMnq1evXiopKdFzzz2njIwM5efnKyIiQn6/X7GxscGTaNdOMTEx8vv9DY5ZXV2t6upq53EgEGjuaQMAAIs0e0CZOnWq83NycrIGDx6sPn36aPfu3Ro7dmyTxszLy9OSJUuaa4oAAMByIb/MuHfv3urWrZuKi4slSV6vVxUVFUF96urqdP78+Ruet5Kbm6uqqipnOXXqVKinDQAAWlDIA8qXX36pc+fOKT4+XpLk8/lUWVmpgoICp8+uXbtUX1+v1NTUBsdwuVxyu91BCwAAaLsa/RHPxYsXnaMhklRaWqrCwkLFxMQoJiZGS5YsUWZmprxer0pKSvSLX/xC9957r9LT0yVJAwYM0Pjx4zVnzhytXr1atbW1ys7O1tSpU7mCBwAASGrCEZTDhw9r6NChGjp0qCQpJydHQ4cO1aJFixQREaEjR47o4YcfVt++fTV79mylpKTo448/lsvlcsZYv369+vfvr7Fjx2rChAkaNWqUfv/73zdfVQAAoFVr9BGUMWPGyBhzw/Zt27bdcoyYmBht2LChsS8NAADuEnwXDwAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDqNDih79+7VQw89pISEBIWFhWnz5s1B7cYYLVq0SPHx8erYsaPS0tJ08uTJoD7nz5/X9OnT5Xa7FR0drdmzZ+vixYt3VAgAAGg7Gh1QLl26pCFDhujVV19tsP2ll17S7373O61evVoHDx5U586dlZ6eritXrjh9pk+frmPHjmn79u3asmWL9u7dq7lz5za9CgAA0Ka0a+wTMjIylJGR0WCbMUYvv/yynn/+eU2aNEmS9G//9m+Ki4vT5s2bNXXqVH322WfaunWrPvnkEw0bNkyS9Morr2jChAn6zW9+o4SEhDsoBwAAtAXNeg5KaWmp/H6/0tLSnHUej0epqanKz8+XJOXn5ys6OtoJJ5KUlpam8PBwHTx4sMFxq6urFQgEghYAANB2NWtA8fv9kqS4uLig9XFxcU6b3+9XbGxsUHu7du0UExPj9PmmvLw8eTweZ0lMTGzOaQMAAMu0iqt4cnNzVVVV5SynTp1q6SkBAIAQataA4vV6JUnl5eVB68vLy502r9erioqKoPa6ujqdP3/e6fNNLpdLbrc7aAEAAG1XswaUXr16yev1aufOnc66QCCggwcPyufzSZJ8Pp8qKytVUFDg9Nm1a5fq6+uVmpranNMBAACtVKOv4rl48aKKi4udx6WlpSosLFRMTIySkpL0zDPP6Ne//rXuu+8+9erVSy+88IISEhL0yCOPSJIGDBig8ePHa86cOVq9erVqa2uVnZ2tqVOncgUPAACQ1ISAcvjwYf3d3/2d8zgnJ0eSNGvWLK1bt06/+MUvdOnSJc2dO1eVlZUaNWqUtm7dqg4dOjjPWb9+vbKzszV27FiFh4crMzNTv/vd75qhHAAA0BaEGWNMS0+isQKBgDwej6qqqkJyPso9v3yv2ccMtT8vn9jSUwAA4KYa8/7dKq7iAQAAdxcCCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHWaPaC8+OKLCgsLC1r69+/vtF+5ckVZWVnq2rWrunTposzMTJWXlzf3NAAAQCsWkiMo3/nOd3TmzBln2bdvn9O2YMECvfvuu9q0aZP27Nmj06dPa/LkyaGYBgAAaKXahWTQdu3k9XqvW19VVaU1a9Zow4YNevDBByVJa9eu1YABA3TgwAGNGDEiFNMBAACtTEiOoJw8eVIJCQnq3bu3pk+frrKyMklSQUGBamtrlZaW5vTt37+/kpKSlJ+ff8PxqqurFQgEghYAANB2NXtASU1N1bp167R161atWrVKpaWl+sEPfqALFy7I7/crMjJS0dHRQc+Ji4uT3++/4Zh5eXnyeDzOkpiY2NzTBgAAFmn2j3gyMjKcnwcPHqzU1FT17NlTf/zjH9WxY8cmjZmbm6ucnBzncSAQIKQAANCGhfwy4+joaPXt21fFxcXyer2qqalRZWVlUJ/y8vIGz1m5xuVyye12By0AAKDtCnlAuXjxokpKShQfH6+UlBS1b99eO3fudNqLiopUVlYmn88X6qkAAIBWotk/4vn5z3+uhx56SD179tTp06e1ePFiRUREaNq0afJ4PJo9e7ZycnIUExMjt9utp556Sj6fjyt4AACAo9kDypdffqlp06bp3Llz6t69u0aNGqUDBw6oe/fukqSVK1cqPDxcmZmZqq6uVnp6ul577bXmngYAAGjFwowxpqUn0ViBQEAej0dVVVUhOR/lnl++1+xjhtqfl09s6SkAAHBTjXn/5rt4AACAdQgoAADAOgQUAABgHQIKAACwTki+LBCAPTjpG0BrxBEUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADW4SoetBiuLgEA3AhHUAAAgHU4ggI0Qms86gMArRFHUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArMNlxm0El78CANoSjqAAAADrEFAAAIB1+IgHgHVa40eWfE8T0Lw4ggIAAKxDQAEAANYhoAAAAOsQUAAAgHU4SRYAmgEn9gLNiyMoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrcB8UALhLce8W2IwjKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOtxJFgCAEGqNd+yVWv6uvRxBAQAA1mnRIyivvvqqVqxYIb/fryFDhuiVV17R8OHDW3JKAACLtdajEWi8FjuC8tZbbyknJ0eLFy/Wf/7nf2rIkCFKT09XRUVFS00JAABYosUCym9/+1vNmTNHTzzxhAYOHKjVq1erU6dOev3111tqSgAAwBIt8hFPTU2NCgoKlJub66wLDw9XWlqa8vPzr+tfXV2t6upq53FVVZUkKRAIhGR+9dWXQzIuAACtRSjeY6+NaYy5Zd8WCShfffWVrl69qri4uKD1cXFxOnHixHX98/LytGTJkuvWJyYmhmyOAADczTwvh27sCxcuyOPx3LRPq7jMODc3Vzk5Oc7j+vp6nT9/Xu3bt1dSUpJOnTolt9vdgjP89gUCASUmJt6VtUt3d/3UTu13W+3S3V1/W6rdGKMLFy4oISHhln1bJKB069ZNERERKi8vD1pfXl4ur9d7XX+XyyWXyxW0Ljo62jlU5Ha7W/1Ga6q7uXbp7q6f2qn9bnQ3199War/VkZNrWuQk2cjISKWkpGjnzp3Ouvr6eu3cuVM+n68lpgQAACzSYh/x5OTkaNasWRo2bJiGDx+ul19+WZcuXdITTzzRUlMCAACWaLGAMmXKFJ09e1aLFi2S3+/Xd7/7XW3duvW6E2dvxuVyafHixdd9/HM3uJtrl+7u+qmd2u9Gd3P9d2vtYeZ2rvUBAAD4FvFdPAAAwDoEFAAAYB0CCgAAsA4BBQAAWKdFA8qrr76qe+65Rx06dFBqaqoOHTp00/6VlZXKyspSfHy8XC6X+vbtq/fff99pf/HFFxUWFha09O/fP2iMK1euKCsrS127dlWXLl2UmZl53Q3jvg3NXfs999xzXe1hYWHKyspy+owZM+a69nnz5oWsxptpTP0NzTssLEwTJ050+hhjtGjRIsXHx6tjx45KS0vTyZMng8Y5f/68pk+fLrfbrejoaM2ePVsXL14MWY030py119bWauHChUpOTlbnzp2VkJCgmTNn6vTp00HjNPT7sXz58pDW2ZDm3u6PP/74de3jx48PGseW7S41f/0NtYeFhWnFihVOn9a47SXp5ZdfVr9+/dSxY0clJiZqwYIFunLlSqPGbK37+1vVnpeXp/vvv19RUVGKjY3VI488oqKioqAxbNrfN5lpIRs3bjSRkZHm9ddfN8eOHTNz5swx0dHRpry8vMH+1dXVZtiwYWbChAlm3759prS01OzevdsUFhY6fRYvXmy+853vmDNnzjjL2bNng8aZN2+eSUxMNDt37jSHDx82I0aMMN///vdDWus3haL2ioqKoLq3b99uJJmPPvrI6fPAAw+YOXPmBPWrqqoKdbnXaWz9586dC5rz0aNHTUREhFm7dq3TZ/ny5cbj8ZjNmzeb//qv/zIPP/yw6dWrl/nrX//q9Bk/frwZMmSIOXDggPn444/Nvffea6ZNmxbqcoM0d+2VlZUmLS3NvPXWW+bEiRMmPz/fDB8+3KSkpASN07NnT7N06dKgsS5evBjqcoOEYrvPmjXLjB8/Pqjf+fPng8axYbsbE5r6v95+5swZ8/rrr5uwsDBTUlLi9GmN2379+vXG5XKZ9evXm9LSUrNt2zYTHx9vFixY0KgxW+P+/nZqT09PN2vXrjVHjx41hYWFZsKECSYpKSlou9qyv78TLRZQhg8fbrKyspzHV69eNQkJCSYvL6/B/qtWrTK9e/c2NTU1Nxxz8eLFZsiQITdsr6ysNO3btzebNm1y1n322WdGksnPz298EU0Uitq/6emnnzZ9+vQx9fX1zroHHnjAPP30002ed3NpbP3ftHLlShMVFeX8MdbX1xuv12tWrFjh9KmsrDQul8u8+eabxhhjjh8/biSZTz75xOnzwQcfmLCwMPOXv/ylOcq6Lc1de0MOHTpkJJkvvvjCWdezZ0+zcuXKJs+7OYSi9lmzZplJkybd8Dm2bHdjvp1tP2nSJPPggw8GrWuN2z4rK+u6OnJycszIkSNve8zWur+/ndq/qaKiwkgye/bscdbZsr+/Ey3yEU9NTY0KCgqUlpbmrAsPD1daWpry8/MbfM6///u/y+fzKSsrS3FxcRo0aJCWLVumq1evBvU7efKkEhIS1Lt3b02fPl1lZWVOW0FBgWpra4Net3///kpKSrrh6za3UNb+9df4wx/+oH/4h39QWFhYUNv69evVrVs3DRo0SLm5ubp8+XLzFXcbmlL/N61Zs0ZTp05V586dJUmlpaXy+/1BY3o8HqWmpjpj5ufnKzo6WsOGDXP6pKWlKTw8XAcPHmyO0m4pFLU3pKqqSmFhYYqOjg5av3z5cnXt2lVDhw7VihUrVFdX16Q6miKUte/evVuxsbHq16+f5s+fr3PnzjltNmx36dvZ9uXl5Xrvvfc0e/bs69pa27b//ve/r4KCAuejkM8//1zvv/++JkyYcNtjttb9/a1qb0hVVZUkKSYmJmh9S+/v71SL3En2q6++0tWrV6+7a2xcXJxOnDjR4HM+//xz7dq1S9OnT9f777+v4uJiPfnkk6qtrdXixYslSampqVq3bp369eunM2fOaMmSJfrBD36go0ePKioqSn6/X5GRkdftuOPi4uT3+0NS6zeFqvav27x5syorK/X4448HrX/sscfUs2dPJSQk6MiRI1q4cKGKior0pz/9qdnqu5Wm1P91hw4d0tGjR7VmzRpn3bVt19CY19r8fr9iY2OD2tu1a6eYmBirt/3XNVT7N125ckULFy7UtGnTgr5U7J/+6Z/0ve99TzExMdq/f79yc3N15swZ/fa3v216QY0QqtrHjx+vyZMnq1evXiopKdFzzz2njIwM5efnKyIiwortLn072/6NN95QVFSUJk+eHLS+NW77xx57TF999ZVGjRolY4zq6uo0b948Pffcc7c9Zmvd39+q9m+qr6/XM888o5EjR2rQoEFB47T0/v5Otdit7hurvr5esbGx+v3vf6+IiAilpKToL3/5i1asWOG8SWdkZDj9Bw8erNTUVPXs2VN//OMfG/y/itbidmr/ujVr1igjI+O6r7OeO3eu83NycrLi4+M1duxYlZSUqE+fPiGvozmsWbNGycnJGj58eEtP5Vt3q9pra2v14x//WMYYrVq1KqgtJyfH+Xnw4MGKjIzUP/7jPyovL69V3D77RrVPnTrV+Tk5OVmDBw9Wnz59tHv3bo0dO/bbnmbI3M7v/euvv67p06erQ4cOQetb47bfvXu3li1bptdee02pqakqLi7W008/rV/96ld64YUXWnp6IdXY2rOysnT06FHt27cvaH1b2N+3yEc83bp1U0RExHVnU5eXl8vr9Tb4nPj4ePXt21cRERHOugEDBsjv96umpqbB50RHR6tv374qLi6WJHm9XtXU1KiysvK2X7e5hbr2L774Qjt27NBPf/rTW84lNTVVkpx/n29DU+q/5tKlS9q4ceN1YfPa8242ptfrVUVFRVB7XV2dzp8/b/W2v+ZGtV9zLZx88cUX2r59+y2/kj01NVV1dXX685//3KgamiqUtX9d79691a1bt6C/+Zbe7lLo6//4449VVFR023/3tm/7F154QTNmzNBPf/pTJScn60c/+pGWLVumvLw81dfX39aYrXV/f6vavy47O1tbtmzRRx99pB49etx0Li2xv79TLRJQIiMjlZKSop07dzrr6uvrtXPnTvl8vgafM3LkSBUXFwdtoP/+7/9WfHy8IiMjG3zOxYsXVVJSovj4eElSSkqK2rdvH/S6RUVFKisru+HrNrdQ17527VrFxsYGXYp4I4WFhZLk/Pt8G5pS/zWbNm1SdXW1fvKTnwSt79Wrl7xeb9CYgUBABw8edMb0+XyqrKxUQUGB02fXrl2qr693/nBDLRS1S/8/nJw8eVI7duxQ165dbzmXwsJChYeHX/fxR6iEqvZv+vLLL3Xu3Dnnd9qG7S6Fvv41a9YoJSVFQ4YMueVcWsO2v3z5ssLDg9+erv0PmjHmtsZsrfv7W9V+7b/Z2dl6++23tWvXLvXq1euWc2mJ/f0da6mzczdu3GhcLpdZt26dOX78uJk7d66Jjo42fr/fGGPMjBkzzC9/+Uunf1lZmYmKijLZ2dmmqKjIbNmyxcTGxppf//rXTp+f/exnZvfu3aa0tNT8x3/8h0lLSzPdunUzFRUVTp958+aZpKQks2vXLnP48GHj8/mMz+f79go3oandmP87OzwpKcksXLjwutcsLi42S5cuNYcPHzalpaXmnXfeMb179zajR48ObbENaGz914waNcpMmTKlwTGXL19uoqOjzTvvvGOOHDliJk2a1OBlxkOHDjUHDx40+/btM/fdd1+LXGbcnLXX1NSYhx9+2PTo0cMUFhYGXVJYXV1tjDFm//79ZuXKlaawsNCUlJSYP/zhD6Z79+5m5syZoS32G5q79gsXLpif//znJj8/35SWlpodO3aY733ve+a+++4zV65ccfrZsN2NCc3vvTHGVFVVmU6dOplVq1Zd19Zat/3ixYtNVFSUefPNN83nn39uPvzwQ9OnTx/z4x//+LbHNKZ17u9vp/b58+cbj8djdu/eHfQ3f/nyZWOMXfv7O9FiAcUYY1555RWTlJRkIiMjzfDhw82BAwectgceeMDMmjUrqP/+/ftNamqqcblcpnfv3uaf//mfTV1dndM+ZcoUEx8fbyIjI83f/u3fmilTppji4uKgMf7617+aJ5980vzN3/yN6dSpk/nRj35kzpw5E9I6G9LctRtjzLZt24wkU1RUdN3rlZWVmdGjR5uYmBjjcrnMvffea5599tkWuy6+sfWfOHHCSDIffvhhg+PV19ebF154wcTFxRmXy2XGjh173b/DuXPnzLRp00yXLl2M2+02TzzxhLlw4UKz13YrzVl7aWmpkdTgcu0eOAUFBSY1NdV4PB7ToUMHM2DAALNs2bKgN/FvS3PWfvnyZTNu3DjTvXt30759e9OzZ08zZ86coDcoY+zZ7sY0/++9Mcb867/+q+nYsaOprKy8rq21bvva2lrz4osvmj59+pgOHTqYxMRE8+STT5r/+Z//ue0xjWmd+/vbqf1Gf/PX7pFj2/6+qcKM+X/HjAAAACzBd/EAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYJ3/BSxpe84yjfqTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(model.hist.hist_edges[:, 5].detach().cpu().numpy())\n",
    "# model.hist.hist_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94973b58-ce2a-4248-a7ff-781c91e5d538",
   "metadata": {
    "tags": []
   },
   "source": [
    "# run logistic regression on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959e99e-e971-45ef-a5b7-7b4a9432f71a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"Computing distance matrix\")\n",
    "m, n = qf.size(0), gf.size(0)\n",
    "distmat = np.zeros((m,n))\n",
    "\n",
    "for iq, q in tqdm(enumerate(qf)):\n",
    "    dif = abs(gf - q)\n",
    "    sim = clf.predict_proba(dif)[:, 1]\n",
    "    distmat[iq, :] =  -sim\n",
    "\n",
    "\n",
    "# distmat = distmat.numpy()\n",
    "print(distmat.shape)\n",
    "\n",
    "print(\"Computing CMC and mAP\")\n",
    "cmc, mAP = evaluate(distmat, q_pids, g_pids, q_camids, g_camids)\n",
    "\n",
    "print(\"Results ----------\")\n",
    "print('top1:{:.1%} top5:{:.1%} top10:{:.1%} mAP:{:.1%}'.format(cmc[0],cmc[4],cmc[9],mAP))\n",
    "print(\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7ef05-5ac4-449d-a3d4-ad1880845309",
   "metadata": {},
   "source": [
    "# compute hist distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e061969-bb09-440c-a6e1-864867310171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hist_intersection(hist_1, hist_2):\n",
    "    if hist_1.ndim == 1:\n",
    "        hist_1 = hist_1.view(1, hist_1.shape[0])\n",
    "    if hist_2.ndim == 1:\n",
    "        hist_2 = hist_2.view(1, hist_2.shape[0])\n",
    "        \n",
    "    minima = torch.minimum(hist_1, hist_2)\n",
    "    intersection = torch.true_divide(torch.sum(minima, dim=1), torch.sum(torch.maximum(hist_1, hist_2), dim=1))\n",
    "    return intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f2d32-f6db-4b0f-abdf-7abb05153c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing distance matrix\")\n",
    "m, n = qf.size(0), gf.size(0)\n",
    "distmat = np.zeros((m,n))\n",
    "\n",
    "for iq, q in tqdm(enumerate(qf)):\n",
    "    for ig, g in enumerate(gf[iq:]):\n",
    "        d = hist_intersection(q, g)\n",
    "        distmat[iq, ig] = d\n",
    "        distmat[ig, iq] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4810b3d6-5731-40b2-afdf-1948350c3b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = torch.FloatTensor([[1,10,1,1,1,1,1,1]])\n",
    "v2 = torch.FloatTensor([[1,4,1,1,1,1,1,1]])\n",
    "# v2 = v2.repeat(len(v1), 1)\n",
    "\n",
    "print(hist_intersection(v1, v2))\n",
    "# print(hist_intersection(v2, v1))\n",
    "torch.nn.CosineSimilarity()(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e2408-0d03-4806-a923-d9474f8d2948",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917122f8-a8f5-4c3f-890e-b5e04a748ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "conf.use_linear_to_get_important_features = False\n",
    "conf.print_model_parameters_trainable = False\n",
    "conf.use_linear_to_merge_features = False\n",
    "conf.use_hist = True\n",
    "conf.centers = [0.1, 0.3, 0.5, 0.7, 0.9, 1.1]\n",
    "conf.widths = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "conf.init_hist(\"HistYusufLayer\")\n",
    "args.pretrain = 'logs/row53/best_model.pth.tar'\n",
    "conf.print_model_layers = False\n",
    "\n",
    "print(\"Initializing model: {}\".format(args.arch))\n",
    "seed_everythings()\n",
    "model = models.init_model(name=args.arch, conf=conf, num_classes=dataset.num_train_pids)\n",
    "print(\"Model size: {:.5f}M\".format(sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "modify_model(model, args, conf)\n",
    "print('model loaded ...')\n",
    "\n",
    "qf1, q_pids1, q_camids1, gf1, g_pids1, g_camids1 = extract_test_feature(model, queryloader, galleryloader, use_bn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce5c1bc-3100-4144-ae02-99ed6625c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "print(\"Computing distance matrix\")\n",
    "m, n = qf1.size(0), gf1.size(0)\n",
    "distmat = np.zeros((m,n))\n",
    "\n",
    "for iq, q in tqdm(enumerate(qf1)):\n",
    "    q_repeat = q.repeat(len(gf1), 1)\n",
    "    d = 1 - hist_intersection(q_repeat, gf1)\n",
    "    distmat[iq, :] = d\n",
    "    \n",
    "\n",
    "print(\"Computing CMC and mAP\")\n",
    "cmc, mAP = evaluate(distmat, q_pids, g_pids, q_camids, g_camids)\n",
    "print(\"Results ----------\")\n",
    "print('top1:{:.1%} top5:{:.1%} top10:{:.1%} mAP:{:.1%}'.format(cmc[0],cmc[4],cmc[9],mAP))\n",
    "print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd54590-6d13-4e8f-a62f-3eff26e10d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "conf.use_linear_to_get_important_features = False\n",
    "conf.print_model_parameters_trainable = False\n",
    "conf.use_linear_to_merge_features = False\n",
    "conf.use_hist = True\n",
    "#conf.centers = [0.1, 0.3, 0.5, 0.7, 0.9, 1.1]\n",
    "conf.centers = [0.5, 1.25, 1.75,  2.2, 2.6, 3.0]\n",
    "#conf.widths = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "conf.widths = [0.5, 0.25, 0.25, 0.2, 0.2, 0.2]\n",
    "conf.init_hist(\"HistYusufLayer\")\n",
    "args.pretrain = 'logs/row53/best_model.pth.tar'\n",
    "conf.print_model_layers = False\n",
    "\n",
    "print(\"Initializing model: {}\".format(args.arch))\n",
    "seed_everythings()\n",
    "model = models.init_model(name=args.arch, conf=conf, num_classes=dataset.num_train_pids)\n",
    "print(\"Model size: {:.5f}M\".format(sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "modify_model(model, args, conf)\n",
    "print('model loaded ...')\n",
    "\n",
    "qf1, q_pids1, q_camids1, gf1, g_pids1, g_camids1 = extract_test_feature(model, queryloader, galleryloader, use_bn=False)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "print(\"Computing distance matrix\")\n",
    "m, n = qf1.size(0), gf1.size(0)\n",
    "distmat = np.zeros((m,n))\n",
    "\n",
    "for iq, q in tqdm(enumerate(qf1)):\n",
    "    q_repeat = q.repeat(len(gf1), 1)\n",
    "    d = 1 - hist_intersection(q_repeat, gf1)\n",
    "    distmat[iq, :] = d\n",
    "    \n",
    "\n",
    "print(\"Computing CMC and mAP\")\n",
    "cmc, mAP = evaluate(distmat, q_pids1, g_pids1, q_camids1, g_camids1)\n",
    "print(\"Results ----------\")\n",
    "print('top1:{:.1%} top5:{:.1%} top10:{:.1%} mAP:{:.1%}'.format(cmc[0],cmc[4],cmc[9],mAP))\n",
    "print(\"------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
