{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c168d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c01befa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/File/shamgholi/projects/person_reid/AP3D\n"
     ]
    }
   ],
   "source": [
    "cd {cwd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a7f206",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec97ca76-3d72-4545-b529-51ae6a31fe40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/1tra/shamgholi/miniconda3/envs/reid/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(cwd)\n",
    "import time\n",
    "import math\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from config import Config\n",
    "import models\n",
    "import transforms.spatial_transforms as ST\n",
    "import transforms.temporal_transforms as TT\n",
    "import tools.data_manager as data_manager\n",
    "from tools.video_loader import VideoDataset\n",
    "from tools.utils import Logger\n",
    "from tools.eval_metrics import evaluate\n",
    "from commons import modify_model\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "def seed_everythings():\n",
    "    seed = 1\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "class Args:\n",
    "    #\"/home/hadi/iust/datasets\"\n",
    "    #\"/home/shamgholi/iust/datasets\" \n",
    "    #\"/mnt/File/shamgholi/datasets\"\n",
    "    root = \"/mnt/File/shamgholi/datasets\"\n",
    "    dataset = \"mars\"\n",
    "    workers = 0\n",
    "    height = 256\n",
    "    width = 128\n",
    "    # test_frames = 8\n",
    "    arch = \"ap3dres50\"\n",
    "    resume = \"./\"\n",
    "    pretrain = os.path.join(cwd, \"logs/row41/best_model.pth.tar\")\n",
    "    test_epochs = [240]\n",
    "    distance = \"cosine\" #cosine\n",
    "    gpu = \"0\"\n",
    "    test_batch = 6\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c77a8-d3df-40cc-b7ea-bd117c918e2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Default Test (Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9094fe3-7444-4f9d-91c1-83838120a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, queryloader, galleryloader, use_gpu, args):\n",
    "    # test using 4 frames\n",
    "    since = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    qf, q_pids, q_camids = [], [], []\n",
    "    for batch_idx, (vids, pids, camids) in enumerate(queryloader):\n",
    "        if use_gpu:\n",
    "            vids = vids.cuda()\n",
    "\n",
    "        feat = model(vids)\n",
    "        feat = feat.mean(1)\n",
    "        feat = model.bn(feat)\n",
    "        feat = feat.data.cpu()\n",
    "\n",
    "        qf.append(feat)\n",
    "        q_pids.extend(pids)\n",
    "        q_camids.extend(camids)\n",
    "\n",
    "    qf = torch.cat(qf, 0)\n",
    "    q_pids = np.asarray(q_pids)\n",
    "    q_camids = np.asarray(q_camids)\n",
    "    print(\"Extracted features for query set, obtained {} matrix\".format(qf.shape))\n",
    "\n",
    "    gf, g_pids, g_camids = [], [], []\n",
    "    for batch_idx, (vids, pids, camids) in enumerate(galleryloader):\n",
    "        if use_gpu:\n",
    "            vids = vids.cuda()\n",
    "\n",
    "        feat = model(vids)\n",
    "        feat = feat.mean(1)\n",
    "        feat = model.bn(feat)\n",
    "        feat = feat.data.cpu()\n",
    "\n",
    "        gf.append(feat)\n",
    "        g_pids.extend(pids)\n",
    "        g_camids.extend(camids)\n",
    "\n",
    "    gf = torch.cat(gf, 0)\n",
    "    g_pids = np.asarray(g_pids)\n",
    "    g_camids = np.asarray(g_camids)\n",
    "\n",
    "    if args.dataset == 'mars':\n",
    "        # gallery set must contain query set, otherwise 140 query imgs will not have ground truth.\n",
    "        gf = torch.cat((qf, gf), 0)\n",
    "        g_pids = np.append(q_pids, g_pids)\n",
    "        g_camids = np.append(q_camids, g_camids)\n",
    "\n",
    "    print(\"Extracted features for gallery set, obtained {} matrix\".format(gf.shape))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Extracting features complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print(\"Computing distance matrix\")\n",
    "    m, n = qf.size(0), gf.size(0)\n",
    "    distmat = torch.zeros((m,n))\n",
    "\n",
    "    if args.distance == 'euclidean':\n",
    "        distmat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n",
    "                  torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n",
    "        for i in range(m):\n",
    "            distmat[i:i+1].addmm_(1, -2, qf[i:i+1], gf.t())\n",
    "    else:\n",
    "        q_norm = torch.norm(qf, p=2, dim=1, keepdim=True)\n",
    "        g_norm = torch.norm(gf, p=2, dim=1, keepdim=True)\n",
    "        qf = qf.div(q_norm.expand_as(qf))\n",
    "        gf = gf.div(g_norm.expand_as(gf))\n",
    "        for i in range(m):\n",
    "            distmat[i] = - torch.mm(qf[i:i+1], gf.t())\n",
    "    distmat = distmat.numpy()\n",
    "    \n",
    "    print(\"Computing CMC and mAP\")\n",
    "    cmc, mAP = evaluate(distmat, q_pids, g_pids, q_camids, g_camids)\n",
    "\n",
    "    print(\"Results ----------\")\n",
    "    print('top1:{:.1%} top5:{:.1%} top10:{:.1%} mAP:{:.1%}'.format(cmc[0],cmc[4],cmc[9],mAP))\n",
    "    print(\"------------------\")\n",
    "\n",
    "    return cmc[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def experiment():\n",
    "    print(\"Initializing model: {}\".format(args.arch))\n",
    "    seed_everythings()\n",
    "    model = models.init_model(name=args.arch, conf=conf, num_classes=dataset.num_train_pids)\n",
    "    print(\"Model size: {:.5f}M\".format(sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    modify_model(model, args, conf)\n",
    "    with torch.no_grad():\n",
    "        test(model, queryloader, galleryloader, use_gpu, args)\n",
    "        # mytest(model, queryloader, galleryloader, use_gpu, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57898897-3e93-41d3-86e9-017fd3b4241c",
   "metadata": {},
   "source": [
    "# Prepare Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dae5896-5f30-4187-8b4d-2d3d70bfdd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> MARS loaded\n",
      "Dataset statistics:\n",
      "  ------------------------------\n",
      "  subset   | # ids | # tracklets\n",
      "  ------------------------------\n",
      "  train    |   625 |     8298\n",
      "  query    |   626 |     1980\n",
      "  gallery  |   622 |     9330\n",
      "  ------------------------------\n",
      "  total    |  1247 |    19608\n",
      "  number of images per tracklet: 2 ~ 920, average 59.5\n",
      "  ------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation\n",
    "args = Args()\n",
    "conf = Config()\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = 'cuda' if use_gpu else 'cpu'\n",
    "pin_memory = True if use_gpu else False\n",
    "temporal_transform_test = TT.TemporalBeginCrop()\n",
    "dataset = data_manager.init_dataset(name=args.dataset, root=args.root)\n",
    "spatial_transform_test = ST.Compose([\n",
    "            ST.Scale((args.height, args.width), interpolation=3),\n",
    "            ST.ToTensor(),\n",
    "            ST.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "# return queryloader, galleryloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c67f9-376a-4b14-8795-8f44be615a19",
   "metadata": {},
   "source": [
    "# Base model Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f756ef37-1de9-4af9-be94-de55fd1aec71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/1tra/shamgholi/miniconda3/envs/reid/lib/python3.7/site-packages/torchvision/transforms/transforms.py:279: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model: ap3dres50\n",
      "Model size: 11.75930M\n",
      "pretrain state dict loaded\n",
      "----------\n",
      "Model size: 11.75930M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "259it [01:19,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features for query set, obtained torch.Size([8288, 512]) matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "spatial_transform_train = conf.get_spatial_transform_train(args)\n",
    "temporal_transform_train = TT.TemporalRandomCrop()\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    VideoDataset(dataset.train, spatial_transform=spatial_transform_train, temporal_transform=temporal_transform_test),\n",
    "    batch_size=32, num_workers=args.workers,\n",
    "    pin_memory=pin_memory, drop_last=True)\n",
    "\n",
    "\n",
    "conf.use_linear_to_get_important_features = False\n",
    "conf.print_model_parameters_trainable = False\n",
    "conf.use_linear_to_merge_features = False\n",
    "conf.use_hist = False\n",
    "args.pretrain = 'logs/row41/best_model.pth.tar'\n",
    "conf.print_model_layers = False\n",
    "\n",
    "print(\"Initializing model: {}\".format(args.arch))\n",
    "seed_everythings()\n",
    "model = models.init_model(name=args.arch, conf=conf, num_classes=dataset.num_train_pids)\n",
    "print(\"Model size: {:.5f}M\".format(sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "modify_model(model, args, conf)\n",
    "with torch.no_grad():\n",
    "\n",
    "    # test using 4 frames\n",
    "    since = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    tf, t_pids, t_camids = [], [], []\n",
    "    for batch_idx, (vids, pids, camids) in tqdm(enumerate(trainloader)):\n",
    "        if use_gpu:\n",
    "            vids = vids.cuda()\n",
    "\n",
    "        feat = model(vids)\n",
    "        feat = feat.mean(1)\n",
    "        feat = model.bn(feat)\n",
    "        feat = feat.data.cpu()\n",
    "\n",
    "        tf.append(feat)\n",
    "        t_pids.extend(pids)\n",
    "        t_camids.extend(camids)\n",
    "        \n",
    "    tf = torch.cat(tf, 0)\n",
    "    t_pids = np.asarray(t_pids)\n",
    "    t_camids = np.asarray(t_camids)\n",
    "    print(\"Extracted features for query set, obtained {} matrix\".format(tf.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aa4f4a5-673d-4e5f-87b7-c9e8e9f30280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 1), (5, 2), (6, 2), (119, 2), (251, 2)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# res = Counter(t_pids) # equals to list(set(words))\n",
    "# # for k,v in list(zip(res.keys(), res.values()))[:5]:\n",
    "# #     print(k, v)\n",
    "# # res.most_common(10)\n",
    "# [(l,k) for k,l in sorted([(j,i) for i,j in res.items()], reverse=False)][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5881a3-78f6-47ec-ae00-91cc76337e82",
   "metadata": {},
   "source": [
    "# Train logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d37dc2ae-d171-497a-9190-8f871c0c2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_dataset = []\n",
    "logistic_labels = []\n",
    "t_qids_unique = np.unique(t_pids)\n",
    "for i, tid in tqdm(enumerate(t_pids)):\n",
    "    population = [ind for ind, pid in enumerate(t_pids) if pid == tid]\n",
    "    population.remove(i)\n",
    "    # if len(population) > 3:\n",
    "    #     k = 4\n",
    "    # elif len(population) > 2:\n",
    "    #     k = 3\n",
    "    # elif len(population) > 1:\n",
    "    #     k = 2\n",
    "    # else:\n",
    "    #     k = 1\n",
    "    try:\n",
    "        i_sample = random.sample(population=population, k=1)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    feats = tf[i_sample]\n",
    "    for feat in feats:\n",
    "        logistic_dataset.append(abs(feat - tf[i]).unsqueeze(0))\n",
    "        logistic_labels.append(1.0)\n",
    "\n",
    "        \n",
    "for i, tid in tqdm(enumerate(t_pids)):\n",
    "    population = [ind for ind, pid in enumerate(t_pids) if pid != tid]\n",
    "    # if len(population) > 3:\n",
    "    #     k = 4\n",
    "    # elif len(population) > 2:\n",
    "    #     k = 3\n",
    "    # elif len(population) > 1:\n",
    "    #     k = 2\n",
    "    # else:\n",
    "    #     k = 1\n",
    "    try:\n",
    "        i_sample = random.sample(population=population, k=1)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    feats = tf[i_sample]\n",
    "    for feat in feats:\n",
    "        logistic_dataset.append(abs(feat - tf[i]).unsqueeze(0))\n",
    "        logistic_labels.append(0.0)\n",
    "      \n",
    "    \n",
    "    \n",
    "logistic_dataset = torch.cat(logistic_dataset, 0).numpy()\n",
    "logistic_labels = torch.FloatTensor(logistic_labels).numpy()\n",
    "print(logistic_dataset.shape)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.utils import shuffle\n",
    "logistic_dataset_shuff, logistic_labels_shuff = shuffle(logistic_dataset, logistic_labels)\n",
    "\n",
    "x_train, y_train = logistic_dataset_shuff[2000:], logistic_labels_shuff[2000:]\n",
    "x_val, y_val = logistic_dataset_shuff[:2000], logistic_labels_shuff[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec2bb4a2-3725-4dd9-8207-c2d28ac2dfcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9975"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf = svm.SVC(probability=True)\n",
    "# clf = LogisticRegression(random_state=1)\n",
    "clf.fit(x_train, y_train)\n",
    "clf.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68015bb-d42a-4143-83d1-331defffb1ed",
   "metadata": {},
   "source": [
    "# prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "699942ad-62f2-48b0-bce7-fc8430aa1144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# random.shuffle(dataset.query)\n",
    "# query_train = dataset.query[:900]\n",
    "# query_test = dataset.query[900:]\n",
    "\n",
    "queryloader = DataLoader(\n",
    "    VideoDataset(dataset.query, spatial_transform=spatial_transform_test, temporal_transform=temporal_transform_test),\n",
    "    batch_size=args.test_batch, shuffle=False, num_workers=0,\n",
    "    pin_memory=pin_memory, drop_last=False)\n",
    "galleryloader = DataLoader(\n",
    "    VideoDataset(dataset.gallery, spatial_transform=spatial_transform_test, temporal_transform=temporal_transform_test),\n",
    "    batch_size=args.test_batch, shuffle=False, num_workers=0,\n",
    "    pin_memory=pin_memory, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf019d0-667a-4e84-ae23-98a34a4c9fc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# test set feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e320a3db-adde-429b-92dc-3869919c960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_feature(model, queryloader, galleryloader, use_bn=True):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # test using 4 frames\n",
    "        since = time.time()\n",
    "        model.eval()\n",
    "\n",
    "        qf, q_pids, q_camids = [], [], []\n",
    "        for batch_idx, (vids, pids, camids) in enumerate(queryloader):\n",
    "            if use_gpu:\n",
    "                vids = vids.cuda()\n",
    "\n",
    "            feat = model(vids)\n",
    "            feat = feat.mean(1)\n",
    "            if use_bn:\n",
    "                feat = model.bn(feat)\n",
    "            feat = feat.data.cpu()\n",
    "\n",
    "            qf.append(feat)\n",
    "            q_pids.extend(pids)\n",
    "            q_camids.extend(camids)\n",
    "\n",
    "        qf = torch.cat(qf, 0)\n",
    "        q_pids = np.asarray(q_pids)\n",
    "        q_camids = np.asarray(q_camids)\n",
    "        print(\"Extracted features for query set, obtained {} matrix\".format(qf.shape))\n",
    "\n",
    "        gf, g_pids, g_camids = [], [], []\n",
    "        for batch_idx, (vids, pids, camids) in enumerate(galleryloader):\n",
    "            if use_gpu:\n",
    "                vids = vids.cuda()\n",
    "\n",
    "            feat = model(vids)\n",
    "            feat = feat.mean(1)\n",
    "            if use_bn:\n",
    "                feat = model.bn(feat)\n",
    "            feat = feat.data.cpu()\n",
    "\n",
    "            gf.append(feat)\n",
    "            g_pids.extend(pids)\n",
    "            g_camids.extend(camids)\n",
    "\n",
    "        gf = torch.cat(gf, 0)\n",
    "        g_pids = np.asarray(g_pids)\n",
    "        g_camids = np.asarray(g_camids)\n",
    "\n",
    "        if args.dataset == 'mars':\n",
    "            # gallery set must contain query set, otherwise 140 query imgs will not have ground truth.\n",
    "            gf = torch.cat((qf, gf), 0)\n",
    "            g_pids = np.append(q_pids, g_pids)\n",
    "            g_camids = np.append(q_camids, g_camids)\n",
    "\n",
    "        print(\"Extracted features for gallery set, obtained {} matrix\".format(gf.shape))\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Extracting features complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    return qf, q_pids, q_camids, gf, g_pids, g_camids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08d08cd2-76c6-4a12-a13f-ba561cf458e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_train_loader = DataLoader(\n",
    "#     VideoDataset(query_train, spatial_transform=spatial_transform_test, temporal_transform=temporal_transform_test),\n",
    "#     batch_size=args.test_batch, shuffle=False, num_workers=0,\n",
    "#     pin_memory=pin_memory, drop_last=False)\n",
    "\n",
    "conf.use_linear_to_get_important_features = False\n",
    "conf.print_model_parameters_trainable = False\n",
    "conf.use_linear_to_merge_features = False\n",
    "conf.use_hist = False\n",
    "args.pretrain = 'logs/row41/best_model.pth.tar'\n",
    "conf.print_model_layers = False\n",
    "\n",
    "print(\"Initializing model: {}\".format(args.arch))\n",
    "seed_everythings()\n",
    "model = models.init_model(name=args.arch, conf=conf, num_classes=dataset.num_train_pids)\n",
    "print(\"Model size: {:.5f}M\".format(sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "modify_model(model, args, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87877d-70f6-4a73-bf0a-fe8f95564c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf, q_pids, q_camids, gf, g_pids, g_camids = extract_test_feature(model, queryloader, galleryloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f6ce6-ea82-4460-aa6d-9c1bbfc872e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## base model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16ea9f78-cf9d-40e9-824b-f48a619792aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing distance matrix\n",
      "Computing CMC and mAP\n",
      "Results ----------\n",
      "top1:83.5% top5:93.1% top10:95.1% mAP:73.2%\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing distance matrix\")\n",
    "m, n = qf.size(0), gf.size(0)\n",
    "distmat = torch.zeros((m,n))\n",
    "\n",
    "if args.distance == 'euclidean':\n",
    "    distmat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n",
    "              torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n",
    "    for i in range(m):\n",
    "        distmat[i:i+1].addmm_(1, -2, qf[i:i+1], gf.t())\n",
    "else:\n",
    "    q_norm = torch.norm(qf, p=2, dim=1, keepdim=True)\n",
    "    g_norm = torch.norm(gf, p=2, dim=1, keepdim=True)\n",
    "    qf = qf.div(q_norm.expand_as(qf))\n",
    "    gf = gf.div(g_norm.expand_as(gf))\n",
    "    for i in range(m):\n",
    "        distmat[i] = - torch.mm(qf[i:i+1], gf.t())\n",
    "distmat = distmat.numpy()\n",
    "\n",
    "print(\"Computing CMC and mAP\")\n",
    "cmc, mAP = evaluate(distmat, q_pids, g_pids, q_camids, g_camids)\n",
    "\n",
    "print(\"Results ----------\")\n",
    "print('top1:{:.1%} top5:{:.1%} top10:{:.1%} mAP:{:.1%}'.format(cmc[0],cmc[4],cmc[9],mAP))\n",
    "print(\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94973b58-ce2a-4248-a7ff-781c91e5d538",
   "metadata": {
    "tags": []
   },
   "source": [
    "# run logistic regression on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d959e99e-e971-45ef-a5b7-7b4a9432f71a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing distance matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1980it [1:02:58,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980, 11310)\n",
      "Computing CMC and mAP\n",
      "Results ----------\n",
      "top1:0.2% top5:0.4% top10:0.4% mAP:0.5%\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"Computing distance matrix\")\n",
    "m, n = qf.size(0), gf.size(0)\n",
    "distmat = np.zeros((m,n))\n",
    "\n",
    "for iq, q in tqdm(enumerate(qf)):\n",
    "    dif = abs(gf - q)\n",
    "    sim = clf.predict_proba(dif)[:, 1]\n",
    "    distmat[iq, :] =  -sim\n",
    "\n",
    "\n",
    "# distmat = distmat.numpy()\n",
    "print(distmat.shape)\n",
    "\n",
    "print(\"Computing CMC and mAP\")\n",
    "cmc, mAP = evaluate(distmat, q_pids, g_pids, q_camids, g_camids)\n",
    "\n",
    "print(\"Results ----------\")\n",
    "print('top1:{:.1%} top5:{:.1%} top10:{:.1%} mAP:{:.1%}'.format(cmc[0],cmc[4],cmc[9],mAP))\n",
    "print(\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7ef05-5ac4-449d-a3d4-ad1880845309",
   "metadata": {},
   "source": [
    "# compute hist distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e061969-bb09-440c-a6e1-864867310171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/1tra/shamgholi/miniconda3/envs/reid/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hist_intersection(hist_1, hist_2):\n",
    "    if hist_1.ndim == 1:\n",
    "        hist_1 = hist_1.view(1, hist_1.shape[0])\n",
    "    if hist_2.ndim == 1:\n",
    "        hist_2 = hist_2.view(1, hist_2.shape[0])\n",
    "        \n",
    "    minima = torch.minimum(hist_1, hist_2)\n",
    "    intersection = torch.true_divide(torch.sum(minima, dim=1), torch.sum(torch.maximum(hist_1, hist_2), dim=1))\n",
    "    return intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f2d32-f6db-4b0f-abdf-7abb05153c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing distance matrix\")\n",
    "m, n = qf.size(0), gf.size(0)\n",
    "distmat = np.zeros((m,n))\n",
    "\n",
    "for iq, q in tqdm(enumerate(qf)):\n",
    "    for ig, g in enumerate(gf[iq:]):\n",
    "        d = hist_intersection(q, g)\n",
    "        distmat[iq, ig] = d\n",
    "        distmat[ig, iq] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4810b3d6-5731-40b2-afdf-1948350c3b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7059])\n",
      "tensor([0.7059])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9741])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = torch.FloatTensor([[1,10,1,1,1,1,1,1]])\n",
    "v2 = torch.FloatTensor([[1,5,1,1,1,1,1,1]])\n",
    "# v2 = v2.repeat(len(v1), 1)\n",
    "\n",
    "print(hist_intersection(v1, v2))\n",
    "print(hist_intersection(v2, v1))\n",
    "torch.nn.CosineSimilarity()(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e2408-0d03-4806-a923-d9474f8d2948",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "917122f8-a8f5-4c3f-890e-b5e04a748ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model: ap3dres50\n",
      "Model size: 13.37671M\n",
      "pretrain state dict loaded\n",
      "----------\n",
      "Model size: 13.37671M\n",
      "model loaded ...\n",
      "Extracted features for query set, obtained torch.Size([1980, 3072]) matrix\n",
      "Extracted features for gallery set, obtained torch.Size([11310, 3072]) matrix\n",
      "Extracting features complete in 1m 44s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "conf.use_linear_to_get_important_features = False\n",
    "conf.print_model_parameters_trainable = False\n",
    "conf.use_linear_to_merge_features = False\n",
    "conf.use_hist = True\n",
    "conf.centers = [0.1, 0.3, 0.5, 0.7, 0.9, 1.1]\n",
    "conf.widths = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "conf.init_hist(\"HistYusufLayer\")\n",
    "args.pretrain = 'logs/row53/best_model.pth.tar'\n",
    "conf.print_model_layers = False\n",
    "\n",
    "print(\"Initializing model: {}\".format(args.arch))\n",
    "seed_everythings()\n",
    "model = models.init_model(name=args.arch, conf=conf, num_classes=dataset.num_train_pids)\n",
    "print(\"Model size: {:.5f}M\".format(sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "modify_model(model, args, conf)\n",
    "print('model loaded ...')\n",
    "\n",
    "qf1, q_pids1, q_camids1, gf1, g_pids1, g_camids1 = extract_test_feature(model, queryloader, galleryloader, use_bn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ce5c1bc-3100-4144-ae02-99ed6625c607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing distance matrix\n",
      "Computing CMC and mAP\n",
      "Results ----------\n",
      "top1:69.9% top5:84.0% top10:87.8% mAP:52.4%\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "print(\"Computing distance matrix\")\n",
    "m, n = qf1.size(0), gf1.size(0)\n",
    "distmat = np.zeros((m,n))\n",
    "\n",
    "for iq, q in tqdm(enumerate(qf1)):\n",
    "    q_repeat = q.repeat(len(gf1), 1)\n",
    "    d = 1 - hist_intersection(q_repeat, gf1)\n",
    "    distmat[iq, :] = d\n",
    "    \n",
    "\n",
    "print(\"Computing CMC and mAP\")\n",
    "cmc, mAP = evaluate(distmat, q_pids, g_pids, q_camids, g_camids)\n",
    "print(\"Results ----------\")\n",
    "print('top1:{:.1%} top5:{:.1%} top10:{:.1%} mAP:{:.1%}'.format(cmc[0],cmc[4],cmc[9],mAP))\n",
    "print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3fd54590-6d13-4e8f-a62f-3eff26e10d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model: ap3dres50\n",
      "Model size: 13.37671M\n",
      "pretrain state dict loaded\n",
      "----------\n",
      "Model size: 13.37671M\n",
      "model loaded ...\n",
      "Extracted features for query set, obtained torch.Size([1980, 3072]) matrix\n",
      "Extracted features for gallery set, obtained torch.Size([11310, 3072]) matrix\n",
      "Extracting features complete in 1m 42s\n",
      "Computing distance matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1980it [02:02, 16.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing CMC and mAP\n",
      "Results ----------\n",
      "top1:63.2% top5:74.2% top10:76.9% mAP:38.1%\n",
      "------------------\n",
      "CPU times: user 15min 29s, sys: 5min 31s, total: 21min 1s\n",
      "Wall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "conf.use_linear_to_get_important_features = False\n",
    "conf.print_model_parameters_trainable = False\n",
    "conf.use_linear_to_merge_features = False\n",
    "conf.use_hist = True\n",
    "#conf.centers = [0.1, 0.3, 0.5, 0.7, 0.9, 1.1]\n",
    "conf.centers = [0.5, 1.25, 1.75,  2.2, 2.6, 3.0]\n",
    "#conf.widths = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "conf.widths = [0.5, 0.25, 0.25, 0.2, 0.2, 0.2]\n",
    "conf.init_hist(\"HistYusufLayer\")\n",
    "args.pretrain = 'logs/row53/best_model.pth.tar'\n",
    "conf.print_model_layers = False\n",
    "\n",
    "print(\"Initializing model: {}\".format(args.arch))\n",
    "seed_everythings()\n",
    "model = models.init_model(name=args.arch, conf=conf, num_classes=dataset.num_train_pids)\n",
    "print(\"Model size: {:.5f}M\".format(sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "modify_model(model, args, conf)\n",
    "print('model loaded ...')\n",
    "\n",
    "qf1, q_pids1, q_camids1, gf1, g_pids1, g_camids1 = extract_test_feature(model, queryloader, galleryloader, use_bn=False)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "print(\"Computing distance matrix\")\n",
    "m, n = qf1.size(0), gf1.size(0)\n",
    "distmat = np.zeros((m,n))\n",
    "\n",
    "for iq, q in tqdm(enumerate(qf1)):\n",
    "    q_repeat = q.repeat(len(gf1), 1)\n",
    "    d = 1 - hist_intersection(q_repeat, gf1)\n",
    "    distmat[iq, :] = d\n",
    "    \n",
    "\n",
    "print(\"Computing CMC and mAP\")\n",
    "cmc, mAP = evaluate(distmat, q_pids1, g_pids1, q_camids1, g_camids1)\n",
    "print(\"Results ----------\")\n",
    "print('top1:{:.1%} top5:{:.1%} top10:{:.1%} mAP:{:.1%}'.format(cmc[0],cmc[4],cmc[9],mAP))\n",
    "print(\"------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
