{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c168d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c01befa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/File/shamgholi/projects/person_reid/AP3D\n"
     ]
    }
   ],
   "source": [
    "cd {cwd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a7f206",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec97ca76-3d72-4545-b529-51ae6a31fe40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/1tra/shamgholi/miniconda3/envs/reid/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(cwd)\n",
    "import time\n",
    "import math\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from config import Config\n",
    "import models\n",
    "import transforms.spatial_transforms as ST\n",
    "import transforms.temporal_transforms as TT\n",
    "import tools.data_manager as data_manager\n",
    "from tools.video_loader import VideoDataset\n",
    "from tools.utils import Logger\n",
    "from tools.eval_metrics import evaluate\n",
    "from commons import modify_model\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "def seed_everythings():\n",
    "    seed = 1\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "class Args:\n",
    "    #\"/home/hadi/iust/datasets\"\n",
    "    #\"/home/shamgholi/iust/datasets\" \n",
    "    #\"/mnt/File/shamgholi/datasets\"\n",
    "    root = \"/mnt/File/shamgholi/datasets\"\n",
    "    dataset = \"mars\"\n",
    "    workers = 0\n",
    "    height = 256\n",
    "    width = 128\n",
    "    # test_frames = 8\n",
    "    arch = \"ap3dres50\"\n",
    "    resume = \"./\"\n",
    "    pretrain = os.path.join(cwd, \"logs/row41/best_model.pth.tar\")\n",
    "    test_epochs = [240]\n",
    "    distance = \"cosine\" #cosine\n",
    "    gpu = \"0\"\n",
    "    test_batch = 6\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c77a8-d3df-40cc-b7ea-bd117c918e2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Default Test (Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9094fe3-7444-4f9d-91c1-83838120a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, queryloader, galleryloader, use_gpu, args):\n",
    "    # test using 4 frames\n",
    "    since = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    qf, q_pids, q_camids = [], [], []\n",
    "    for batch_idx, (vids, pids, camids) in enumerate(queryloader):\n",
    "        if use_gpu:\n",
    "            vids = vids.cuda()\n",
    "\n",
    "        feat = model(vids)\n",
    "        feat = feat.mean(1)\n",
    "        feat = model.bn(feat)\n",
    "        feat = feat.data.cpu()\n",
    "\n",
    "        qf.append(feat)\n",
    "        q_pids.extend(pids)\n",
    "        q_camids.extend(camids)\n",
    "\n",
    "    qf = torch.cat(qf, 0)\n",
    "    q_pids = np.asarray(q_pids)\n",
    "    q_camids = np.asarray(q_camids)\n",
    "    print(\"Extracted features for query set, obtained {} matrix\".format(qf.shape))\n",
    "\n",
    "    gf, g_pids, g_camids = [], [], []\n",
    "    for batch_idx, (vids, pids, camids) in enumerate(galleryloader):\n",
    "        if use_gpu:\n",
    "            vids = vids.cuda()\n",
    "\n",
    "        feat = model(vids)\n",
    "        feat = feat.mean(1)\n",
    "        feat = model.bn(feat)\n",
    "        feat = feat.data.cpu()\n",
    "\n",
    "        gf.append(feat)\n",
    "        g_pids.extend(pids)\n",
    "        g_camids.extend(camids)\n",
    "\n",
    "    gf = torch.cat(gf, 0)\n",
    "    g_pids = np.asarray(g_pids)\n",
    "    g_camids = np.asarray(g_camids)\n",
    "\n",
    "    if args.dataset == 'mars':\n",
    "        # gallery set must contain query set, otherwise 140 query imgs will not have ground truth.\n",
    "        gf = torch.cat((qf, gf), 0)\n",
    "        g_pids = np.append(q_pids, g_pids)\n",
    "        g_camids = np.append(q_camids, g_camids)\n",
    "\n",
    "    print(\"Extracted features for gallery set, obtained {} matrix\".format(gf.shape))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Extracting features complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print(\"Computing distance matrix\")\n",
    "    m, n = qf.size(0), gf.size(0)\n",
    "    distmat = torch.zeros((m,n))\n",
    "\n",
    "    if args.distance == 'euclidean':\n",
    "        distmat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n",
    "                  torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n",
    "        for i in range(m):\n",
    "            distmat[i:i+1].addmm_(1, -2, qf[i:i+1], gf.t())\n",
    "    else:\n",
    "        q_norm = torch.norm(qf, p=2, dim=1, keepdim=True)\n",
    "        g_norm = torch.norm(gf, p=2, dim=1, keepdim=True)\n",
    "        qf = qf.div(q_norm.expand_as(qf))\n",
    "        gf = gf.div(g_norm.expand_as(gf))\n",
    "        for i in range(m):\n",
    "            distmat[i] = - torch.mm(qf[i:i+1], gf.t())\n",
    "    distmat = distmat.numpy()\n",
    "    \n",
    "    print(\"Computing CMC and mAP\")\n",
    "    cmc, mAP = evaluate(distmat, q_pids, g_pids, q_camids, g_camids)\n",
    "\n",
    "    print(\"Results ----------\")\n",
    "    print('top1:{:.1%} top5:{:.1%} top10:{:.1%} mAP:{:.1%}'.format(cmc[0],cmc[4],cmc[9],mAP))\n",
    "    print(\"------------------\")\n",
    "\n",
    "    return cmc[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def experiment():\n",
    "    print(\"Initializing model: {}\".format(args.arch))\n",
    "    seed_everythings()\n",
    "    model = models.init_model(name=args.arch, conf=conf, num_classes=dataset.num_train_pids)\n",
    "    print(\"Model size: {:.5f}M\".format(sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    modify_model(model, args, conf)\n",
    "    with torch.no_grad():\n",
    "        test(model, queryloader, galleryloader, use_gpu, args)\n",
    "        # mytest(model, queryloader, galleryloader, use_gpu, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57898897-3e93-41d3-86e9-017fd3b4241c",
   "metadata": {},
   "source": [
    "# Prepare Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dae5896-5f30-4187-8b4d-2d3d70bfdd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> MARS loaded\n",
      "Dataset statistics:\n",
      "  ------------------------------\n",
      "  subset   | # ids | # tracklets\n",
      "  ------------------------------\n",
      "  train    |   625 |     8298\n",
      "  query    |   626 |     1980\n",
      "  gallery  |   622 |     9330\n",
      "  ------------------------------\n",
      "  total    |  1247 |    19608\n",
      "  number of images per tracklet: 2 ~ 920, average 59.5\n",
      "  ------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation\n",
    "args = Args()\n",
    "conf = Config()\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = 'cuda' if use_gpu else 'cpu'\n",
    "pin_memory = True if use_gpu else False\n",
    "temporal_transform_test = TT.TemporalBeginCrop()\n",
    "dataset = data_manager.init_dataset(name=args.dataset, root=args.root)\n",
    "spatial_transform_test = ST.Compose([\n",
    "            ST.Scale((args.height, args.width), interpolation=3),\n",
    "            ST.ToTensor(),\n",
    "            ST.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "# return queryloader, galleryloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c67f9-376a-4b14-8795-8f44be615a19",
   "metadata": {},
   "source": [
    "# Base model Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f756ef37-1de9-4af9-be94-de55fd1aec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_transform_train = conf.get_spatial_transform_train(args)\n",
    "temporal_transform_train = TT.TemporalRandomCrop()\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    VideoDataset(dataset.train, spatial_transform=spatial_transform_train, temporal_transform=temporal_transform_test),\n",
    "    batch_size=32, num_workers=args.workers,\n",
    "    pin_memory=pin_memory, drop_last=True)\n",
    "\n",
    "\n",
    "conf.use_linear_to_get_important_features = False\n",
    "conf.print_model_parameters_trainable = False\n",
    "conf.use_linear_to_merge_features = False\n",
    "conf.use_hist = False\n",
    "args.pretrain = 'logs/row41/best_model.pth.tar'\n",
    "conf.print_model_layers = False\n",
    "\n",
    "print(\"Initializing model: {}\".format(args.arch))\n",
    "seed_everythings()\n",
    "model = models.init_model(name=args.arch, conf=conf, num_classes=dataset.num_train_pids)\n",
    "print(\"Model size: {:.5f}M\".format(sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "modify_model(model, args, conf)\n",
    "with torch.no_grad():\n",
    "\n",
    "    # test using 4 frames\n",
    "    since = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    tf, t_pids, t_camids = [], [], []\n",
    "    for batch_idx, (vids, pids, camids) in tqdm(enumerate(trainloader)):\n",
    "        if use_gpu:\n",
    "            vids = vids.cuda()\n",
    "\n",
    "        feat = model(vids)\n",
    "        feat = feat.mean(1)\n",
    "        feat = model.bn(feat)\n",
    "        feat = feat.data.cpu()\n",
    "\n",
    "        tf.append(feat)\n",
    "        t_pids.extend(pids)\n",
    "        t_camids.extend(camids)\n",
    "        \n",
    "    tf = torch.cat(tf, 0)\n",
    "    t_pids = np.asarray(t_pids)\n",
    "    t_camids = np.asarray(t_camids)\n",
    "    print(\"Extracted features for query set, obtained {} matrix\".format(tf.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aa4f4a5-673d-4e5f-87b7-c9e8e9f30280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 1), (5, 2), (6, 2), (119, 2), (251, 2)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "res = Counter(t_pids) # equals to list(set(words))\n",
    "# for k,v in list(zip(res.keys(), res.values()))[:5]:\n",
    "#     print(k, v)\n",
    "# res.most_common(10)\n",
    "[(l,k) for k,l in sorted([(j,i) for i,j in res.items()], reverse=False)][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5881a3-78f6-47ec-ae00-91cc76337e82",
   "metadata": {},
   "source": [
    "# Train logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0ad8bef6-2b7e-41d6-9f02-e7a1cd6afd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8288it [00:10, 768.55it/s]\n",
      "8288it [00:12, 654.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16575, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.995"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_dataset = []\n",
    "logistic_labels = []\n",
    "t_qids_unique = np.unique(t_pids)\n",
    "for i, tid in tqdm(enumerate(t_pids)):\n",
    "    population = [ind for ind, pid in enumerate(t_pids) if pid == tid]\n",
    "    population.remove(i)\n",
    "    # if len(population) > 3:\n",
    "    #     k = 4\n",
    "    # elif len(population) > 2:\n",
    "    #     k = 3\n",
    "    # elif len(population) > 1:\n",
    "    #     k = 2\n",
    "    # else:\n",
    "    #     k = 1\n",
    "    try:\n",
    "        i_sample = random.sample(population=population, k=1)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    feats = tf[i_sample]\n",
    "    for feat in feats:\n",
    "        logistic_dataset.append(abs(feat - tf[i]).unsqueeze(0))\n",
    "        logistic_labels.append(1.0)\n",
    "\n",
    "        \n",
    "for i, tid in tqdm(enumerate(t_pids)):\n",
    "    population = [ind for ind, pid in enumerate(t_pids) if pid != tid]\n",
    "    # if len(population) > 3:\n",
    "    #     k = 4\n",
    "    # elif len(population) > 2:\n",
    "    #     k = 3\n",
    "    # elif len(population) > 1:\n",
    "    #     k = 2\n",
    "    # else:\n",
    "    #     k = 1\n",
    "    try:\n",
    "        i_sample = random.sample(population=population, k=1)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    feats = tf[i_sample]\n",
    "    for feat in feats:\n",
    "        logistic_dataset.append(abs(feat - tf[i]).unsqueeze(0))\n",
    "        logistic_labels.append(0.0)\n",
    "      \n",
    "    \n",
    "    \n",
    "logistic_dataset = torch.cat(logistic_dataset, 0).numpy()\n",
    "logistic_labels = torch.FloatTensor(logistic_labels).numpy()\n",
    "print(logistic_dataset.shape)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "logistic_dataset_shuff, logistic_labels_shuff = shuffle(logistic_dataset, logistic_labels)\n",
    "\n",
    "x_train, y_train = logistic_dataset_shuff[200:], logistic_labels_shuff[200:]\n",
    "x_val, y_val = logistic_dataset_shuff[:200], logistic_labels_shuff[:200]\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# x_train = scaler.fit_transform(x_train)\n",
    "# x_val = scaler.transform(x_val)\n",
    "\n",
    "clf = LogisticRegression(random_state=1, max_iter=1000).fit(x_train, y_train)\n",
    "clf.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68015bb-d42a-4143-83d1-331defffb1ed",
   "metadata": {},
   "source": [
    "# prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "699942ad-62f2-48b0-bce7-fc8430aa1144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# random.shuffle(dataset.query)\n",
    "# query_train = dataset.query[:900]\n",
    "# query_test = dataset.query[900:]\n",
    "\n",
    "queryloader = DataLoader(\n",
    "    VideoDataset(dataset.query, spatial_transform=spatial_transform_test, temporal_transform=temporal_transform_test),\n",
    "    batch_size=args.test_batch, shuffle=False, num_workers=0,\n",
    "    pin_memory=pin_memory, drop_last=False)\n",
    "galleryloader = DataLoader(\n",
    "    VideoDataset(dataset.gallery, spatial_transform=spatial_transform_test, temporal_transform=temporal_transform_test),\n",
    "    batch_size=args.test_batch, shuffle=False, num_workers=0,\n",
    "    pin_memory=pin_memory, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f6ce6-ea82-4460-aa6d-9c1bbfc872e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## base model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16ea9f78-cf9d-40e9-824b-f48a619792aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model: ap3dres50\n",
      "Model size: 11.75930M\n",
      "pretrain state dict loaded\n",
      "----------\n",
      "Model size: 11.75930M\n",
      "Extracted features for query set, obtained torch.Size([1980, 512]) matrix\n",
      "Extracted features for gallery set, obtained torch.Size([11310, 512]) matrix\n",
      "Extracting features complete in 1m 40s\n",
      "Computing distance matrix\n",
      "Computing CMC and mAP\n",
      "Results ----------\n",
      "top1:83.5% top5:93.1% top10:95.1% mAP:73.2%\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "conf.use_linear_to_get_important_features = False\n",
    "conf.print_model_parameters_trainable = False\n",
    "conf.use_linear_to_merge_features = False\n",
    "conf.use_hist = False\n",
    "args.pretrain = 'logs/row41/best_model.pth.tar'\n",
    "conf.print_model_layers = False\n",
    "experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf019d0-667a-4e84-ae23-98a34a4c9fc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# test set feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "46ed381a-6207-4ad1-acb0-6b1c25710ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model: ap3dres50\n",
      "Model size: 11.75930M\n",
      "pretrain state dict loaded\n",
      "----------\n",
      "Model size: 11.75930M\n",
      "Extracted features for query set, obtained torch.Size([1980, 512]) matrix\n",
      "Extracted features for gallery set, obtained torch.Size([11310, 512]) matrix\n",
      "Extracting features complete in 2m 5s\n"
     ]
    }
   ],
   "source": [
    "# query_train_loader = DataLoader(\n",
    "#     VideoDataset(query_train, spatial_transform=spatial_transform_test, temporal_transform=temporal_transform_test),\n",
    "#     batch_size=args.test_batch, shuffle=False, num_workers=0,\n",
    "#     pin_memory=pin_memory, drop_last=False)\n",
    "\n",
    "conf.use_linear_to_get_important_features = False\n",
    "conf.print_model_parameters_trainable = False\n",
    "conf.use_linear_to_merge_features = False\n",
    "conf.use_hist = False\n",
    "args.pretrain = 'logs/row41/best_model.pth.tar'\n",
    "conf.print_model_layers = False\n",
    "\n",
    "print(\"Initializing model: {}\".format(args.arch))\n",
    "seed_everythings()\n",
    "model = models.init_model(name=args.arch, conf=conf, num_classes=dataset.num_train_pids)\n",
    "print(\"Model size: {:.5f}M\".format(sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "modify_model(model, args, conf)\n",
    "with torch.no_grad():\n",
    "\n",
    "    # test using 4 frames\n",
    "    since = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    qf, q_pids, q_camids = [], [], []\n",
    "    for batch_idx, (vids, pids, camids) in enumerate(queryloader):\n",
    "        if use_gpu:\n",
    "            vids = vids.cuda()\n",
    "\n",
    "        feat = model(vids)\n",
    "        feat = feat.mean(1)\n",
    "        feat = model.bn(feat)\n",
    "        feat = feat.data.cpu()\n",
    "\n",
    "        qf.append(feat)\n",
    "        q_pids.extend(pids)\n",
    "        q_camids.extend(camids)\n",
    "\n",
    "    qf = torch.cat(qf, 0)\n",
    "    q_pids = np.asarray(q_pids)\n",
    "    q_camids = np.asarray(q_camids)\n",
    "    print(\"Extracted features for query set, obtained {} matrix\".format(qf.shape))\n",
    "\n",
    "    gf, g_pids, g_camids = [], [], []\n",
    "    for batch_idx, (vids, pids, camids) in enumerate(galleryloader):\n",
    "        if use_gpu:\n",
    "            vids = vids.cuda()\n",
    "\n",
    "        feat = model(vids)\n",
    "        feat = feat.mean(1)\n",
    "        feat = model.bn(feat)\n",
    "        feat = feat.data.cpu()\n",
    "\n",
    "        gf.append(feat)\n",
    "        g_pids.extend(pids)\n",
    "        g_camids.extend(camids)\n",
    "\n",
    "    gf = torch.cat(gf, 0)\n",
    "    g_pids = np.asarray(g_pids)\n",
    "    g_camids = np.asarray(g_camids)\n",
    "\n",
    "    if args.dataset == 'mars':\n",
    "        # gallery set must contain query set, otherwise 140 query imgs will not have ground truth.\n",
    "        gf = torch.cat((qf, gf), 0)\n",
    "        g_pids = np.append(q_pids, g_pids)\n",
    "        g_camids = np.append(q_camids, g_camids)\n",
    "\n",
    "    print(\"Extracted features for gallery set, obtained {} matrix\".format(gf.shape))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Extracting features complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94973b58-ce2a-4248-a7ff-781c91e5d538",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# run logistic regression on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ff9825f-f7f5-4def-b8fc-5132c81e0688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing distance matrix\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing distance matrix\")\n",
    "m, n = qf.size(0), gf.size(0)\n",
    "distmat = torch.zeros((m,n))\n",
    "\n",
    "if args.distance == 'euclidean':\n",
    "    distmat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n",
    "              torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n",
    "    for i in range(m):\n",
    "        distmat[i:i+1].addmm_(1, -2, qf[i:i+1], gf.t())\n",
    "else:\n",
    "    q_norm = torch.norm(qf, p=2, dim=1, keepdim=True)\n",
    "    g_norm = torch.norm(gf, p=2, dim=1, keepdim=True)\n",
    "    qf = qf.div(q_norm.expand_as(qf))\n",
    "    gf = gf.div(g_norm.expand_as(gf))\n",
    "    for i in range(m):\n",
    "        distmat[i] = - torch.mm(qf[i:i+1], gf.t())\n",
    "distmat = distmat.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d959e99e-e971-45ef-a5b7-7b4a9432f71a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing distance matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1980it [01:08, 28.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980, 11310)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"Computing distance matrix\")\n",
    "m, n = qf.size(0), gf.size(0)\n",
    "distmat = np.zeros((m,n))\n",
    "\n",
    "for iq, q in tqdm(enumerate(qf)):\n",
    "    dif = abs(gf - q)\n",
    "    sim = clf.predict_proba(dif)[:, 1]\n",
    "    distmat[iq, :] = 1 - sim\n",
    "\n",
    "\n",
    "# distmat = distmat.numpy()\n",
    "print(distmat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ffb51df-e34b-4cf6-b858-dc81770ba672",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing CMC and mAP\n",
      "Results ----------\n",
      "top1:0.2% top5:0.4% top10:0.4% mAP:0.5%\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing CMC and mAP\")\n",
    "cmc, mAP = evaluate(distmat, q_pids, g_pids, q_camids, g_camids)\n",
    "\n",
    "print(\"Results ----------\")\n",
    "print('top1:{:.1%} top5:{:.1%} top10:{:.1%} mAP:{:.1%}'.format(cmc[0],cmc[4],cmc[9],mAP))\n",
    "print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64259b72-5331-45fd-919d-cb43634b0d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
