{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c168d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c01befa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/File/shamgholi/projects/person_reid/AP3D\n"
     ]
    }
   ],
   "source": [
    "cd {cwd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a7f206",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec97ca76-3d72-4545-b529-51ae6a31fe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(cwd)\n",
    "import time\n",
    "import math\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from config import Config\n",
    "import models\n",
    "import transforms.spatial_transforms as ST\n",
    "import transforms.temporal_transforms as TT\n",
    "import tools.data_manager as data_manager\n",
    "from tools.video_loader import VideoDataset\n",
    "from tools.utils import Logger\n",
    "from tools.eval_metrics import evaluate\n",
    "from commons import modify_model\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "def seed_everythings():\n",
    "    seed = 1\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "class Args:\n",
    "    #\"/home/hadi/iust/datasets\"\n",
    "    #\"/home/shamgholi/iust/datasets\" \n",
    "    #\"/mnt/File/shamgholi/datasets\"\n",
    "    root = \"/mnt/File/shamgholi/datasets\"\n",
    "    dataset = \"mars\"\n",
    "    workers = 0\n",
    "    height = 256\n",
    "    width = 128\n",
    "    # test_frames = 8\n",
    "    arch = \"ap3dres50\"\n",
    "    resume = \"./\"\n",
    "    pretrain = os.path.join(cwd, \"logs/row41/best_model.pth.tar\")\n",
    "    test_epochs = [240]\n",
    "    distance = \"cosine\" #cosine\n",
    "    gpu = \"0\"\n",
    "    test_batch = 6\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c77a8-d3df-40cc-b7ea-bd117c918e2a",
   "metadata": {},
   "source": [
    "# Default Test (Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9094fe3-7444-4f9d-91c1-83838120a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, queryloader, galleryloader, use_gpu, args):\n",
    "    # test using 4 frames\n",
    "    since = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    qf, q_pids, q_camids = [], [], []\n",
    "    for batch_idx, (vids, pids, camids) in enumerate(queryloader):\n",
    "        if use_gpu:\n",
    "            vids = vids.cuda()\n",
    "\n",
    "        feat = model(vids)\n",
    "        feat = feat.mean(1)\n",
    "        feat = model.bn(feat)\n",
    "        feat = feat.data.cpu()\n",
    "\n",
    "        qf.append(feat)\n",
    "        q_pids.extend(pids)\n",
    "        q_camids.extend(camids)\n",
    "\n",
    "    qf = torch.cat(qf, 0)\n",
    "    q_pids = np.asarray(q_pids)\n",
    "    q_camids = np.asarray(q_camids)\n",
    "    print(\"Extracted features for query set, obtained {} matrix\".format(qf.shape))\n",
    "\n",
    "    gf, g_pids, g_camids = [], [], []\n",
    "    for batch_idx, (vids, pids, camids) in enumerate(galleryloader):\n",
    "        if use_gpu:\n",
    "            vids = vids.cuda()\n",
    "\n",
    "        feat = model(vids)\n",
    "        feat = feat.mean(1)\n",
    "        feat = model.bn(feat)\n",
    "        feat = feat.data.cpu()\n",
    "\n",
    "        gf.append(feat)\n",
    "        g_pids.extend(pids)\n",
    "        g_camids.extend(camids)\n",
    "\n",
    "    gf = torch.cat(gf, 0)\n",
    "    g_pids = np.asarray(g_pids)\n",
    "    g_camids = np.asarray(g_camids)\n",
    "\n",
    "    if args.dataset == 'mars':\n",
    "        # gallery set must contain query set, otherwise 140 query imgs will not have ground truth.\n",
    "        gf = torch.cat((qf, gf), 0)\n",
    "        g_pids = np.append(q_pids, g_pids)\n",
    "        g_camids = np.append(q_camids, g_camids)\n",
    "\n",
    "    print(\"Extracted features for gallery set, obtained {} matrix\".format(gf.shape))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Extracting features complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print(\"Computing distance matrix\")\n",
    "    m, n = qf.size(0), gf.size(0)\n",
    "    distmat = torch.zeros((m,n))\n",
    "\n",
    "    if args.distance == 'euclidean':\n",
    "        distmat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n",
    "                  torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n",
    "        for i in range(m):\n",
    "            distmat[i:i+1].addmm_(1, -2, qf[i:i+1], gf.t())\n",
    "    else:\n",
    "        q_norm = torch.norm(qf, p=2, dim=1, keepdim=True)\n",
    "        g_norm = torch.norm(gf, p=2, dim=1, keepdim=True)\n",
    "        qf = qf.div(q_norm.expand_as(qf))\n",
    "        gf = gf.div(g_norm.expand_as(gf))\n",
    "        for i in range(m):\n",
    "            distmat[i] = - torch.mm(qf[i:i+1], gf.t())\n",
    "    distmat = distmat.numpy()\n",
    "    \n",
    "    print(\"Computing CMC and mAP\")\n",
    "    cmc, mAP = evaluate(distmat, q_pids, g_pids, q_camids, g_camids)\n",
    "\n",
    "    print(\"Results ----------\")\n",
    "    print('top1:{:.1%} top5:{:.1%} top10:{:.1%} mAP:{:.1%}'.format(cmc[0],cmc[4],cmc[9],mAP))\n",
    "    print(\"------------------\")\n",
    "\n",
    "    return cmc[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def experiment():\n",
    "    print(\"Initializing model: {}\".format(args.arch))\n",
    "    seed_everythings()\n",
    "    model = models.init_model(name=args.arch, conf=conf, num_classes=dataset.num_train_pids)\n",
    "    print(\"Model size: {:.5f}M\".format(sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    modify_model(model, args, conf)\n",
    "    with torch.no_grad():\n",
    "        test(model, queryloader, galleryloader, use_gpu, args)\n",
    "        # mytest(model, queryloader, galleryloader, use_gpu, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c5972-03c6-4896-b1df-0b060b242c49",
   "metadata": {},
   "source": [
    "# My Test (Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f0dcb95-ea2b-47b5-b949-8c8a082b157f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mytest(model, queryloader, galleryloader, use_gpu, args):  \n",
    "    # test using 4 frames\n",
    "    since = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    qf, q_pids, q_camids = [], [], []\n",
    "    for batch_idx, (vids, pids, camids) in enumerate(queryloader):\n",
    "        if use_gpu:\n",
    "            vids = vids.cuda()\n",
    "\n",
    "        feat = model(vids)\n",
    "        feat = feat.mean(1)\n",
    "        # feat = model.bn(feat)\n",
    "        feat = feat.data.cpu()\n",
    "\n",
    "        qf.append(feat)\n",
    "        q_pids.extend(pids)\n",
    "        q_camids.extend(camids)\n",
    "\n",
    "    qf = torch.cat(qf, 0)\n",
    "    q_pids = np.asarray(q_pids)\n",
    "    q_camids = np.asarray(q_camids)\n",
    "    print(\"Extracted features for query set, obtained {} matrix\".format(qf.shape))\n",
    "\n",
    "    gf, g_pids, g_camids = [], [], []\n",
    "    for batch_idx, (vids, pids, camids) in enumerate(galleryloader):\n",
    "        if use_gpu:\n",
    "            vids = vids.cuda()\n",
    "\n",
    "        feat = model(vids)\n",
    "        feat = feat.mean(1)\n",
    "        # feat = model.bn(feat)\n",
    "        feat = feat.data.cpu()\n",
    "\n",
    "        gf.append(feat)\n",
    "        g_pids.extend(pids)\n",
    "        g_camids.extend(camids)\n",
    "\n",
    "    gf = torch.cat(gf, 0)\n",
    "    g_pids = np.asarray(g_pids)\n",
    "    g_camids = np.asarray(g_camids)\n",
    "\n",
    "    if args.dataset == 'mars':\n",
    "        # gallery set must contain query set, otherwise 140 query imgs will not have ground truth.\n",
    "        gf = torch.cat((qf, gf), 0)\n",
    "        g_pids = np.append(q_pids, g_pids)\n",
    "        g_camids = np.append(q_camids, g_camids)\n",
    "\n",
    "    print(\"Extracted features for gallery set, obtained {} matrix\".format(gf.shape))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Extracting features complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print(\"Computing distance matrix\")\n",
    "    m, n = qf.size(0), gf.size(0)\n",
    "    distmat = torch.zeros((m,n))\n",
    "\n",
    "    if args.distance == 'euclidean':\n",
    "        distmat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n",
    "                  torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n",
    "        for i in range(m):\n",
    "            distmat[i:i+1].addmm_(1, -2, qf[i:i+1], gf.t())\n",
    "    else:\n",
    "        q_norm = torch.norm(qf, p=2, dim=1, keepdim=True)\n",
    "        g_norm = torch.norm(gf, p=2, dim=1, keepdim=True)\n",
    "        qf = qf.div(q_norm.expand_as(qf))\n",
    "        gf = gf.div(g_norm.expand_as(gf))\n",
    "        for i in range(m):\n",
    "            distmat[i] = - torch.mm(qf[i:i+1], gf.t())\n",
    "    distmat = distmat.numpy()\n",
    "    \n",
    "    print(\"Computing CMC and mAP\")\n",
    "    cmc, mAP = evaluate(distmat, q_pids, g_pids, q_camids, g_camids)\n",
    "\n",
    "    print(\"Results ----------\")\n",
    "    print('top1:{:.1%} top5:{:.1%} top10:{:.1%} mAP:{:.1%}'.format(cmc[0],cmc[4],cmc[9],mAP))\n",
    "    print(\"------------------\")\n",
    "\n",
    "    return cmc[0]\n",
    "\n",
    "\n",
    "def myexperiment():\n",
    "    print(\"Initializing model: {}\".format(args.arch))\n",
    "    seed_everythings()\n",
    "    model = models.init_model(name=args.arch, conf=conf, num_classes=dataset.num_train_pids)\n",
    "    print(\"Model size: {:.5f}M\".format(sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    modify_model(model, args, conf)\n",
    "    with torch.no_grad():\n",
    "        # test(model, queryloader, galleryloader, use_gpu, args)\n",
    "        mytest(model, queryloader, galleryloader, use_gpu, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57898897-3e93-41d3-86e9-017fd3b4241c",
   "metadata": {},
   "source": [
    "# prepare date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dae5896-5f30-4187-8b4d-2d3d70bfdd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> MARS loaded\n",
      "Dataset statistics:\n",
      "  ------------------------------\n",
      "  subset   | # ids | # tracklets\n",
      "  ------------------------------\n",
      "  train    |   625 |     8298\n",
      "  query    |   626 |     1980\n",
      "  gallery  |   622 |     9330\n",
      "  ------------------------------\n",
      "  total    |  1247 |    19608\n",
      "  number of images per tracklet: 2 ~ 920, average 59.5\n",
      "  ------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Data augmentation\n",
    "args = Args()\n",
    "conf = Config()\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = 'cuda' if use_gpu else 'cpu'\n",
    "pin_memory = True if use_gpu else False\n",
    "temporal_transform_test = TT.TemporalBeginCrop()\n",
    "dataset = data_manager.init_dataset(name=args.dataset, root=args.root)\n",
    "spatial_transform_test = ST.Compose([\n",
    "            ST.Scale((args.height, args.width), interpolation=3),\n",
    "            ST.ToTensor(),\n",
    "            ST.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "queryloader = DataLoader(\n",
    "    VideoDataset(dataset.query, spatial_transform=spatial_transform_test, temporal_transform=temporal_transform_test),\n",
    "    batch_size=args.test_batch, shuffle=False, num_workers=0,\n",
    "    pin_memory=pin_memory, drop_last=False)\n",
    "galleryloader = DataLoader(\n",
    "    VideoDataset(dataset.gallery, spatial_transform=spatial_transform_test, temporal_transform=temporal_transform_test),\n",
    "    batch_size=args.test_batch, shuffle=False, num_workers=0,\n",
    "    pin_memory=pin_memory, drop_last=False)\n",
    "\n",
    "# return queryloader, galleryloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab052e51-9bb5-4d51-97a8-0f8ff2eb029a",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f667cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf.use_linear_to_get_important_features = False\n",
    "#conf.print_model_parameters_trainable = False\n",
    "# conf.use_linear_to_merge_features = False\n",
    "# conf.use_hist = False\n",
    "# args.pretrain = 'logs/row41/best_model.pth.tar'\n",
    "# conf.print_model_layers = False\n",
    "# myexperiment()\n",
    "##top1:79.3% top5:91.6% top10:94.0% mAP:69.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff1dceeb-244e-4e46-ba99-cf6e3ba7db38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 14.66900M\n",
      "--------------------\n",
      "RUNTIMEERROR IN LOADING BATCHNORM STATEDICT, WEIGHTS OF BN IS NOW RANDOM\n",
      "--------------------\n",
      "pretrain state dict loaded\n",
      "----------\n",
      "Model size: 14.66900M\n",
      "Extracted features for query set, obtained torch.Size([1980, 5120]) matrix\n",
      "Extracted features for gallery set, obtained torch.Size([11310, 5120]) matrix\n",
      "Extracting features complete in 1m 43s\n",
      "Computing distance matrix\n",
      "Computing CMC and mAP\n",
      "Results ----------\n",
      "top1:62.6% top5:77.2% top10:82.4% mAP:41.9%\n",
      "------------------\n",
      "CPU times: user 10min 47s, sys: 40.6 s, total: 11min 27s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "conf.use_linear_to_get_important_features = False\n",
    "conf.print_model_parameters_trainable = False\n",
    "conf.use_linear_to_merge_features = False\n",
    "conf.use_hist = True\n",
    "conf.concat_hist_max = False\n",
    "conf.init_hist(\"HistYusufLayer\")\n",
    "conf.print_model_layers = False\n",
    "args.pretrain = 'logs/row41/best_model.pth.tar'\n",
    "seed_everythings()\n",
    "model = models.init_model(name=args.arch, conf=conf, num_classes=dataset.num_train_pids)\n",
    "# print(\"Model size: {:.5f}M\".format(sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "modify_model(model, args, conf)\n",
    "with torch.no_grad():\n",
    "    # test(model, queryloader, galleryloader, use_gpu, args)\n",
    "    mytest(model, queryloader, galleryloader, use_gpu, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67e91799-3d1f-4b19-93d7-aa81e6c6c0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: if root path is changed, the previously generated json files need to be re-generated (so delete them first)\n",
      "=> Automatically generating split (might take a while for the first time, have a coffe)\n",
      "Processing /mnt/File/shamgholi/datasets/DukeMTMC-VideoReID/train with 702 person identities\n",
      "Saving split to /mnt/File/shamgholi/datasets/DukeMTMC-VideoReID/split_train.json\n",
      "=> Automatically generating split (might take a while for the first time, have a coffe)\n",
      "Processing /mnt/File/shamgholi/datasets/DukeMTMC-VideoReID/train with 702 person identities\n",
      "Saving split to /mnt/File/shamgholi/datasets/DukeMTMC-VideoReID/split_train_dense.json\n",
      "=> Automatically generating split (might take a while for the first time, have a coffe)\n",
      "Processing /mnt/File/shamgholi/datasets/DukeMTMC-VideoReID/query with 702 person identities\n",
      "Saving split to /mnt/File/shamgholi/datasets/DukeMTMC-VideoReID/split_query.json\n",
      "=> Automatically generating split (might take a while for the first time, have a coffe)\n",
      "Processing /mnt/File/shamgholi/datasets/DukeMTMC-VideoReID/gallery with 1110 person identities\n",
      "Warn: index name F0001 in /mnt/File/shamgholi/datasets/DukeMTMC-VideoReID/gallery/0002/2197 is missing, jump to next\n",
      "Saving split to /mnt/File/shamgholi/datasets/DukeMTMC-VideoReID/split_gallery.json\n",
      "the number of tracklets under dense sampling for train set: 4761\n",
      "=> DukeMTMC-VideoReID loaded\n",
      "Dataset statistics:\n",
      "  ------------------------------\n",
      "  subset   | # ids | # tracklets\n",
      "  ------------------------------\n",
      "  train    |   702 |     2196\n",
      "  query    |   702 |      702\n",
      "  gallery  |  1110 |     2636\n",
      "  ------------------------------\n",
      "  total    |  1404 |     5534\n",
      "  number of images per tracklet: 1 ~ 9324, average 167.6\n",
      "  ------------------------------\n"
     ]
    }
   ],
   "source": [
    "temporal_transform_test = TT.TemporalBeginCrop()\n",
    "args.dataset = \"duke\"\n",
    "dataset = data_manager.init_dataset(name=args.dataset, root=args.root)\n",
    "spatial_transform_test = ST.Compose([\n",
    "            ST.Scale((args.height, args.width), interpolation=3),\n",
    "            ST.ToTensor(),\n",
    "            ST.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "queryloader = DataLoader(\n",
    "    VideoDataset(dataset.query, spatial_transform=spatial_transform_test, temporal_transform=temporal_transform_test),\n",
    "    batch_size=args.test_batch, shuffle=False, num_workers=0,\n",
    "    pin_memory=pin_memory, drop_last=False)\n",
    "galleryloader = DataLoader(\n",
    "    VideoDataset(dataset.gallery, spatial_transform=spatial_transform_test, temporal_transform=temporal_transform_test),\n",
    "    batch_size=args.test_batch, shuffle=False, num_workers=0,\n",
    "    pin_memory=pin_memory, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "420b3d71-957b-48c2-b88f-7a23c6b4c98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model: ap3dres50\n",
      "Model size: 11.79881M\n",
      "pretrain state dict loaded\n",
      "----------\n",
      "Model size: 11.79881M\n",
      "Extracted features for query set, obtained torch.Size([702, 512]) matrix\n",
      "Extracted features for gallery set, obtained torch.Size([2636, 512]) matrix\n",
      "Extracting features complete in 0m 44s\n",
      "Computing distance matrix\n",
      "Computing CMC and mAP\n",
      "Results ----------\n",
      "top1:25.8% top5:40.9% top10:48.7% mAP:23.5%\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "conf.use_linear_to_get_important_features = False\n",
    "conf.print_model_parameters_trainable = False\n",
    "conf.use_linear_to_merge_features = False\n",
    "conf.use_hist = False\n",
    "args.pretrain = 'logs/row41/best_model.pth.tar'\n",
    "conf.print_model_layers = False\n",
    "experiment()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
